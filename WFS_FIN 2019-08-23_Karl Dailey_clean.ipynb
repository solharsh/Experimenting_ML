{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "## Findings\r\n\r\n* 7 Models Finished Rank 2\r\n* 6 were XG Boost w/ Weather Data\r\n* 1 was an ensemble ((y_hat_xgb^1 + y_hat_xgb^2 + y_hat_xgb^3 +y_hat_xgb^4 )/4 ) * .75 + events * .25\r\n* Most submisions were rank 4 including a plain Logistic Regression, and top public leaderboard submission\r\n* Trust your Cross Validation" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "A blog post on Medium describes how you can use powers to improve ROC in ensembles:\n[Reaching the depths of (power/geometric) ensembling when targeting the AUC metric](https://medium.com/data-design/reaching-the-depths-of-power-geometric-ensembling-when-targeting-the-auc-metric-2f356ea3250e)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\neider.env.getUploaded(\"weather.png\", \"/tmp/weather.png\")\r\nimg=mpimg.imread('/tmp/weather.png')\r\n\r\nplt.figure(figsize=(20, 15))\r\nimgplot = plt.imshow(img)\r\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "eider.env.getUploaded(\"xgboost.jpg\", \"/tmp/xgboost.jpg\")\r\nimg=mpimg.imread('/tmp/xgboost.jpg')\r\nimgplot = plt.imshow(img)\r\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "############################################################\n## Download Data from S3\n############################################################\n\n## Training Data\neider.env.getUploaded(\"WFS_Training.csv\", \"/tmp/WFS_Training.csv\")\n\n## Test Data\neider.env.getUploaded(\"WFS_TestFeatures.csv\", \"/tmp/WFS_TestFeatures.csv\")\n\n## Weather Data\neider.env.getUploaded(\"weather.csv\", \"/tmp/weather.csv\")\n\n###############################################################################\n## Imports\n###############################################################################\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## Models\n#from catboost import CatBoostClassifier ## Not in Eider =(\n#import lightgbm as lgb ## Not in Eider =(\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\n\n## Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\n## processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\n\n## Other\nfrom datetime import datetime\nimport os\nimport pickle\nfrom sklearn.model_selection import KFold\n\n###############################################################################\n## settings, functions and globals\n###############################################################################\npd.set_option('display.max_columns', None)\n\nPATH = 'tmp/'\n\ntarget = 'nhenoshow_flag'\n\ndef plot_category_percent_of_target(data, col):\n    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n    cat_percent = data[[col, target]].groupby(col, as_index=False).mean()\n    cat_size    = data[col].value_counts().reset_index(drop=False)\n    cat_size.columns = [col, 'count']\n    cat_percent = cat_percent.merge(cat_size, on=col, how='left')\n    cat_percent[target] = cat_percent[target].fillna(0)\n    cat_percent = cat_percent.sort_values(by='count', ascending=False)[:20]\n    sns.barplot(ax=ax, x=target, y=col, data=cat_percent, order=cat_percent[col])\n\n    for i, p in enumerate(ax.patches):\n        ax.annotate('{}'.format(cat_percent['count'].values[i]), (p.get_width(), p.get_y()+0.5), fontsize=20)\n\n    plt.xlabel('% of ' + target + '(target)')\n    plt.ylabel(col)\n    #plt.savefig(PATH + 'category_percent.' + str(f) + '.jpg')\n    plt.show()\n    plt.close()\n\ndef fix_shift_days_week(df):\n    df.loc[df.shift_days_of_week.isnull(),'shift_days_of_week'] = ''\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace(' ', '')\n    df['shift_days_of_week'] = df.shift_days_of_week.str.lower()\n    \n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('thursday' , 'thu')\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('friday'   , 'fri')\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('saturday' , 'sat')\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('sunday'   , 'sun')\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('monday'   , 'mon')\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('tuesday'  , 'tue')\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('wednesday', 'wed')\n    \n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('tues', 'tue')\n    \n    ## remove slashes\n    df.loc[df.shift_days_of_week=='m/tu/th/f', 'shift_days_of_week'] = 'mon,tue,thu,fri'\n    df.loc[df.shift_days_of_week=='w/th/f/su', 'shift_days_of_week'] = 'wed,thu,fri,sun'\n    df.loc[df.shift_days_of_week=='m/t/th/f', 'shift_days_of_week'] = 'mon,tue,thu,fri'\n    df.loc[df.shift_days_of_week=='m/tu/th/f/sa', 'shift_days_of_week'] = 'mon,tue,thu,fri,sat'\n    df.loc[df.shift_days_of_week=='mon-wed/f/sa', 'shift_days_of_week'] = 'mon,tue,wed,fri,sat'\n    df['shift_days_of_week'] = df.shift_days_of_week.str.replace('/', ',')\n    \n    ## remove dashes\n    booll = df.shift_days_of_week.str.contains('mon-wed')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('mon-wed','mon,tue,wed')\n    booll = df.shift_days_of_week.str.contains('mon-fri')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('mon-fri','mon,tue,wed,thu,fri')\n    booll = df.shift_days_of_week.str.contains('mon-thu')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('mon-thu','mon,tue,wed,thu')\n    booll = df.shift_days_of_week.str.contains('mon-wed')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('mon-wed','mon,tue,wed')\n    booll = df.shift_days_of_week.str.contains('tue-sat')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('tue-sat','tue,wed,thu,fri,sat')\n    booll = df.shift_days_of_week.str.contains('tue-fri')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('tue-fri','tue,wed,thu,fri')\n    booll = df.shift_days_of_week.str.contains('tue-thu')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('tue-thu','tue,wed,thu')\n    booll = df.shift_days_of_week.str.contains('tue-wed')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('tue-wed','tue,wed')\n    booll = df.shift_days_of_week.str.contains('wed-sun')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('wed-sun','wed,thu,fri,sat,sun')\n    booll = df.shift_days_of_week.str.contains('wed-sat')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('wed-sat','wed,thu,fri,sat')\n    booll = df.shift_days_of_week.str.contains('thu-mon')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('thu-mon','thu,fri,sat,sun,mon')\n    booll = df.shift_days_of_week.str.contains('thu-sun')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('thu-sun','thu,fri,sat,sun')\n    booll = df.shift_days_of_week.str.contains('thu-sat')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('thu-sat','thu,fri,sat')\n    booll = df.shift_days_of_week.str.contains('fri-tue')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('fri-tue','fri,sat,sun,mon,tue')\n    booll = df.shift_days_of_week.str.contains('fri-mon')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('fri-mon','fri,sat,sun,mon')\n    booll = df.shift_days_of_week.str.contains('fri-sun')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('fri-sun','fri,sat,sun')\n    booll = df.shift_days_of_week.str.contains('sat-wed')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('sat-wed','sat,sun,mon,tue,wed')\n    booll = df.shift_days_of_week.str.contains('sat-tue')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('sat-tue','sat,sun,mon,tue')\n    booll = df.shift_days_of_week.str.contains('sat-mon')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('sat-mon','sat,sun,mon')\n    booll = df.shift_days_of_week.str.contains('sun-thu')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('sun-thu','sun,mon,tue,wed,thu')\n    booll = df.shift_days_of_week.str.contains('sun-wed')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('sun-wed','sun,mon,tue,wed')\n    booll = df.shift_days_of_week.str.contains('sun-tue')\n    df.loc[booll, 'shift_days_of_week'] =  df[booll].shift_days_of_week.str.replace('sun-tue','sun,mon,tue')\n\n###############################################################################\n## Read Data\n###############################################################################\nprint('Reading Data...')\ndf_train = pd.read_csv(PATH + 'WFS_Training.csv')\ndf_test  = pd.read_csv(PATH + 'WFS_TestFeatures.csv', encoding = \"ISO-8859-1\")\n\n## Shuffle\ndf_train = df_train.sample(frac=1, random_state=2019)\ndf_train.reset_index(drop=True, inplace=True)\n\nprint(' Mean target:', np.mean(df_train[target]))\nprint(\" Done\", datetime.now())\n\n###############################################################################\n## Combine Data & FE\n###############################################################################\nprint('FE...')\n\ndf_test[target]   = -1\ndf_test['train']  = -1\ndf_train['train'] = 0\n## For doing Single Fold Experimentation\nn = int(df_train.shape[0] * .8)\ndf_train.loc[0:n, 'train'] = 1\n\nprint(' Combine...')\ndf_test = df_test[df_train.columns]\ndf_test.reset_index(drop=True, inplace=True)\n\ndf = pd.concat([df_train, df_test], axis=0)\ndf.reset_index(drop=True, inplace=True)\n\nprint(' Add weather...')\nweather = pd.read_csv(PATH +'weather.csv')\nweather.loc[weather.prec=='T','prec'] = 0\nweather['prec'] = weather.prec.astype(float)\n\nweather.loc[weather.snow=='T','prec'] = 0\nweather['snow'] = weather.prec.astype(float)\n\ndf['date'] = df.appt_1_date.str[0:10]\ndf = pd.merge( df, weather, how='left', on='date')\ndf.reset_index(drop=True, inplace=True)\n\ndf.drop('date', inplace=True, axis=1)\n\n# Shift Start Time\ndf['sst'] = 0\nbooll = (~df.shift_start_time.isnull()) & (df.shift_start_time.str.contains('PM'))\ndf.loc[booll, 'sst'] = 12\nfor i in range(0,13):\n    booll = (~df.shift_start_time.isnull()) & (df.shift_start_time.str.contains(str(i)+':'))\n    df.loc[booll, 'sst'] = df.loc[booll, 'sst'] + i\nbooll = ~df.shift_start_time.isnull()\ndf.loc[booll, 'sst']  =  df[booll].sst + df[booll].shift_start_time.str[-5:-2].astype(int)/60\ndf.loc[df.shift_start_time.isnull(), 'sst'] = -1 #np.mean(df.loc[booll, 'sst'] )\n\n## Shift End Time\ndf['set'] = 0\nbooll = (~df.shift_end_time.isnull()) & (df.shift_end_time.str.contains('PM'))\ndf.loc[booll, 'set'] = 12\nfor i in range(0,13):\n    booll = (~df.shift_end_time.isnull()) & (df.shift_end_time.str.contains(str(i)+':'))\n    df.loc[booll, 'set'] = df.loc[booll, 'set'] + i\nbooll = ~df.shift_end_time.isnull()\ndf.loc[booll, 'set']  =  df[booll].set + df[booll].shift_end_time.str[-5:-2].astype(int)/60\ndf.loc[df.shift_end_time.isnull(), 'set'] = -1 #np.mean(df.loc[booll, 'sst'] )\n\n## Shift Total Time\ndf['stt']  = df['set'] + 24  - df['sst'] \ndf.loc[df.stt>24,'stt'] = df[df.stt>24].stt - 24\n\n## Date Fields\ndf['appt_1_date'] = df.appt_1_date.str.replace('Z', '')\ndf['appt_1_date'] = df.appt_1_date.str.replace('T', ' ')\ndf['appt_1_date'] = pd.to_datetime(df.appt_1_date,infer_datetime_format=True)\ndf['app_created_date'] = df.app_created_date.str.replace('Z', '')\ndf['app_created_date'] = df.app_created_date.str.replace('T', ' ')\ndf['app_created_date'] = pd.to_datetime(df.app_created_date,infer_datetime_format=True)\n\n## Dang Eider and its out of date Pandas! >=o\ntry:\n    df['appt_1_day_of_week']    = df['appt_1_date'].dt.day_name()\nexcept:\n    df['appt_1_day_of_week']    = df['appt_1_date'].dt.weekday_name\n\ndf['appt_1_hour']           = df['appt_1_date'].dt.hour\ndf['appt_1_week']           = df['appt_1_date'].dt.week\n\ndf['app_create_to_app_1']   = (df['app_created_date']-df['appt_1_date'])/ pd.offsets.Day(-1)\n\nprint(' Removing shift codes...')\nnarf1 = [f for f in df_train.shift_code.unique() if f not in df_test.shift_code.unique()]\nnarf2 = [f for f in df_test.shift_code.unique() if f not in df_train.shift_code.unique()]\nnarf3 = df_train.shift_code.value_counts().reset_index()\nnarf3 = list(narf3[narf3.shift_code<=3]['index'].unique())\ndf.loc[df.shift_code.isin( list(set( narf1 + narf2 + narf3 )) ), 'shift_code'] = -1\n\nprint(' Fixing shift_days_of_week...')\nfix_shift_days_week(df)\n\n## Add number of workdays\ndf['work_days'] = df.apply(lambda x: len(str(x.shift_days_of_week).split(',')),axis=1)\n\nf='cand_education'\ndf.loc[df.cand_education=='AssociateÄôs / Trade School / Vocational',f] = 'Associate’s / Trade School / Vocational'\ndf.loc[df.cand_education=='Associateâs / Trade School / Vocational',f] = 'Associate’s / Trade School / Vocational'\n\nprint(' Turn cand_assess_overall_score into ordinal...')\nf = 'cand_assess_overall_score'\ndf.loc[df[f]=='Highest'   , f] = 1\ndf.loc[df[f]=='High'      , f] = .75\ndf.loc[df[f]=='Moderate'  , f] = .5\ndf.loc[df[f]=='Low'       , f] = 0\ndf.loc[df[f]=='Ineligible', f] =-1\ndf.loc[df[f].isnull()     , f] =-1\ndf[f] = df[f].astype(int)\n\ndf['app_created_day']      = df['app_created_date'].dt.dayofyear\ndf['appt_1_day'     ]      = df['appt_1_date'     ].dt.dayofyear\n\nnarf = pd.DataFrame(df[df.train!=-1].groupby(by=['appt_1_day'])[target].count().reset_index())\nnarf.rename( {'nhenoshow_flag':'appt_1_day_apts'} , axis=1, inplace=True)\ndf = pd.merge( df, narf, how='left', on='appt_1_day')\ndf.reset_index(drop=True, inplace=True)\n\nnarf = pd.DataFrame(df[df.train!=-1].groupby(by=['appt_1_date'])[target].count().reset_index())\nnarf.rename( {'nhenoshow_flag':'appt_1_date_apts'} , axis=1, inplace=True)\ndf = pd.merge( df, narf, how='left', on='appt_1_date')\ndf.reset_index(drop=True, inplace=True)\ndf.appt_1_date_apts.fillna(0, inplace=True)\n\nprint(' One hot encoding app_esl_status...')\ndf_esl = pd.get_dummies(df.app_esl_status,dummy_na=True)\ndf_esl.columns= ['app_esl_status_ESL', 'app_esl_status_NonESL', 'app_esl_status_ESLNAN']\ndf = pd.concat( [df, df_esl], axis=1)\ndf.drop('app_esl_status', inplace=True, axis=1)\ndf.reset_index(drop=True, inplace=True)\n\nprint(' One hot encoding shift_schedule_type...')\nf = 'shift_schedule_type'\ndf.loc[df[f]=='Flex Time (<19 hours)'     , f] = 'flex'\ndf.loc[df[f]=='Full-Time'                 , f] = 'full'\ndf.loc[df[f]=='Part-Time (20-29 hours)'   , f] = 'part'\ndf.loc[df[f]=='Reduced Time (30-39 hours)', f] = 'reduced'\ndf.loc[df[f].isnull()     , f] = 'Other'\ndf_sctype = pd.get_dummies(df.shift_schedule_type,dummy_na=False)\ndf_sctype.columns= ['shift_schedule_type_' + str(f) for f in list(df_sctype.columns)]\ndf = pd.concat( [df, df_sctype], axis=1)\ndf.drop('shift_schedule_type', inplace=True, axis=1)\ndf.reset_index(drop=True, inplace=True)\n\nprint(' Adding App-ID prefix...')\ndf['app_id'] = df.app_id.str.replace('App-','')\nfor i in range(1,8):\n    df['app_id_' + str(i)] = df.app_id.str[i].astype(int)\n\nprint(' One hot encoding shift_startday...')\ndf_shift_startday = pd.get_dummies(df.shift_startday,dummy_na=False)\ndf_shift_startday.columns= ['shift_startday_' + str(f) for f in list(df_shift_startday.columns)]\ndf = pd.concat( [df, df_shift_startday], axis=1)\ndf.drop('shift_startday', inplace=True, axis=1)\ndf.reset_index(drop=True, inplace=True)\n\nnon_number_columns = df.dtypes[(df.dtypes == object) | (df.dtypes=='datetime64[ns]') | (df.dtypes=='timedelta64[ns]') ].index.values\n\nfor f in non_number_columns:\n    print(f, df[f].value_counts().shape)\n\nprint(' Adding Events...')\ndtemp = df.groupby(['cand_id','app_created_date','appt_1_date']).app_id.count().reset_index()\ndtemp.rename(  {'app_id':'events'}, axis=1, inplace=True)\ndtemp['events'] = 1/dtemp.events\ndf = pd.merge(df, dtemp, how='left', on=['cand_id','app_created_date','appt_1_date'])\ndf.reset_index(drop=True, inplace=True)\n\nprint(' Adding appt_1_date mean ...')\ndtemp2 = df[df.train.isin([0,1])].groupby(['appt_1_date'])[target].mean().reset_index()\ndtemp2.rename(  {'nhenoshow_flag':'appt_1_date_mean'}, axis=1, inplace=True)\ndf = pd.merge(df, dtemp2, how='left', on=['appt_1_date'])\ndf.reset_index(drop=True, inplace=True)\n\n\nprint(\" Done\", datetime.now())" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Numeric Feature Findings\r\n* d_dep looks like it should be helpful\r\n* events should be super helpful\r\n* appt_1_date_mean also looks super helpful\r\n\r\n# Categorical Feature Findings\r\n* the events feature above was not discovered on its own, it was discovered by looking at the categorical feature plots\r\n* cand_id notice the plot, its sorted with high values top and these are all very low target averages yet the average is .78 and mostly '1's\r\n* Mean(target) with events == 1 is .996 while the Mean(target) events < 1 = .41!" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## Some Analasys\r\n###############################################################################\r\nfeatures = [f for f in df.columns if f not in \r\n            ['ID', 'train',target,'app_id','cand_id','shift_end_time','shift_start_time'\r\n            ,'appt_1_day', 'app_created_day'\r\n            ,'app_created_hour']]\r\n\r\nfeatures_lr =     [f for f in features if f not in non_number_columns if f not in ['app_create_to_app_1']]\r\nfeatures_lr_pca = [f for f in features if f not in non_number_columns if f not in ['app_create_to_app_1']]\r\nfeatures_lr_no_w = [f for f in features_lr if f not in weather.columns]\r\nnumber_columns = [f for f in features_lr if f not in non_number_columns]\r\n\r\nfor f in number_columns:\r\n    if df[df.train>=0][f].value_counts(dropna=False).shape[0]>3:\r\n        df_temp = df[df.train>=0][[f,target]].fillna(0)\r\n        sns.distplot(df_temp[df_temp[target]==1][f],color='blue')\r\n        sns.distplot(df_temp[df_temp[target]==0][f],color='red')\r\n        #plt.savefig(PATH + 'histogram.' + str(f) + '.jpg')\r\n        plt.show()\r\n        plt.close()\r\n\r\nfor f in non_number_columns:\r\n    plot_category_percent_of_target(df[df.train>=0], f)\r\n\r\nprint('Mean Target Events = 1: ', np.mean(df[  (df.train>=0) &(df.events==1) ][target]))\r\nprint('Mean Target Events < 1: ', np.mean(df[  (df.train>=0) &(df.events<1) ][target]))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## Finish UP FE\r\n###############################################################################\r\n\r\nprint(' Label Encoding non number columns...')\r\nfor column in non_number_columns:\r\n    print('  ' + column)\r\n    encoder = LabelEncoder().fit(df[column].astype(str))\r\n    df[column] = encoder.transform(df[column].astype(str)).astype(np.int32)\r\n\r\nprint('FE done.')\r\n\r\n###############################################################################\r\n## Parameters\r\n###############################################################################\r\n\r\nn_components = 32\r\npca = PCA(n_components=n_components)\r\npca.fit(  np.nan_to_num(  df[ features_lr_pca ].values  )  ) \r\n\r\nparams_cat = {}\r\nparams_cat['loss_function'] = 'MultiClass'\r\nparams_cat['random_seed'] =   2019\r\nparams_cat['classes_count'] = 2\r\nparams_cat['l2_leaf_reg']   = 3\r\nparams_cat['depth']         = 8\r\nparams_cat['learning_rate'] = 0.05\r\nparams_cat['iterations']    = 250\r\nparams_cat['verbose'] = False\r\n\r\nparams_lgb = {}\r\nparams_lgb['objective']        = 'multiclass'\r\nparams_lgb['max_depth']        = 7\r\nparams_lgb['num_leaves']       = 32\r\nparams_lgb['feature_fraction'] = 0.95\r\nparams_lgb['bagging_fraction'] = 0.8\r\nparams_lgb['bagging_freq']     = 1\r\nparams_lgb['learning_rate']    = 0.05\r\nparams_lgb['verbosity']        = 2\r\nparams_lgb['verbose']          = 2\r\nparams_lgb['num_class']        = 2\r\nparams_lgb['lambda']          = 0.1\r\nparams_lgb['alpha']           = 0.1\r\nparams_lgb['random_state']    = 2019\r\n\r\nX_train = preprocessing.scale(  np.nan_to_num( df[df.train >= 0][ features_lr ].values )  )\r\nY_train = df[df.train >= 0][ target   ].values\r\nX_test  = preprocessing.scale(  np.nan_to_num( df[df.train ==-1][ features_lr ].values )  )\r\n\r\n\r\nEPOCHS  = 5\r\n\r\nX_train2 = preprocessing.scale(  np.nan_to_num( df[df.train >= 0][ features_lr_no_w ].values )  )\r\nX_test2  = preprocessing.scale(  np.nan_to_num( df[df.train ==-1][ features_lr_no_w ].values )  )" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## Base Model\r\n###############################################################################\r\nbaseline = df[df.train>=0][['events',target]].copy()\r\nbaseline.fillna(np.mean(baseline.events), inplace=True)\r\nprint('\\n  Baseline: ', 'AUC:', roc_auc_score( Y_train, baseline.events), 'ACC:', accuracy_score(Y_train, ( baseline.events > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  baseline.events > 0.5  ).astype(int)) )" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## Logistic Regression\r\n###############################################################################\r\nprint('\\n Logistic Regression...')\r\ny_hat_lr = np.zeros(X_test.shape[0])\r\ny_oof_lr = np.zeros(X_train.shape[0])\r\nfold      = 1\r\nkf      = KFold(n_splits = EPOCHS, shuffle = True, random_state=2019)\r\n\r\nfor tr_idx, val_idx in kf.split(X_train, Y_train):\r\n    X_tr, X_vl = X_train[tr_idx], X_train[val_idx, :]\r\n    y_tr, y_vl = Y_train[tr_idx], Y_train[val_idx]\r\n    model_lr = LogisticRegression(random_state = 2019, C=1,tol=.0001).fit(X_tr, y_tr)\r\n \r\n    y_pred_train = model_lr.predict_proba(X_vl)[:,1]\r\n    y_oof_lr[val_idx] = y_pred_train\r\n    \r\n    y_zero = max(np.mean(y_vl), 1-np.mean(y_vl))\r\n    ACC    = accuracy_score(y_vl, (y_pred_train > 0.5  ).astype(int) )\r\n    AUC    = roc_auc_score( y_vl, y_pred_train)\r\n    LIFT   = ( ACC - y_zero )*100\r\n    print('  LR: ', 'AUC:', AUC, 'ACC:', ACC, 'LIFT:', LIFT)\r\n\r\n    y_hat_lr+= model_lr.predict_proba(X_test)[:,1] / EPOCHS\r\n    fold+=1\r\n\r\nprint('  LR AVG: ', 'AUC:', roc_auc_score( Y_train, y_oof_lr), 'ACC:', accuracy_score(Y_train, (y_oof_lr > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  y_oof_lr > 0.5  ).astype(int)) )\r\n\r\ndf_test[target]= y_hat_lr\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.lr.' + 'folds' + str(EPOCHS) + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_lr))\r\n\r\nmodel_lr_full = LogisticRegression(random_state = 2019, C=1,tol=.0001).fit(X_train, Y_train)\r\ny_hat_lr_full = model_lr_full.predict_proba(X_test)[:,1]\r\n\r\ndf_test[target]= y_hat_lr_full\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.lr.' + 'full' + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_lr_full))\r\n\r\nfeature_imp_lr = pd.DataFrame(sorted(zip(model_lr_full.coef_[0],features_lr)), columns=['Value','Feature'])\r\nplt.figure(figsize=(20, 10))\r\nsns.barplot(x=\"Value\", y=\"Feature\", data= feature_imp_lr.sort_values(by=\"Value\", ascending=False))\r\nplt.title('Logistic Regression Feature Importance')\r\nplt.tight_layout()\r\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print('----------------------------------------------------------------------')\r\nfrom sklearn.model_selection import KFold\r\n\r\nprint('Final Models...')\r\nprint(' xgbboost...')\r\nX_train = preprocessing.scale(  np.nan_to_num( df[df.train >= 0][ features_lr ].values )  )\r\nY_train = df[df.train >= 0][ target   ].values\r\nX_test  = preprocessing.scale(  np.nan_to_num( df[df.train ==-1][ features_lr ].values )  )\r\nEPOCHS  = 5\r\n\r\nparams_xgb = {}\r\nparams_xgb['eta']         = .037\r\nparams_xgb['seed']        = 2019\r\nparams_xgb['objective']   = 'multi:softprob'\r\nparams_xgb['num_class']   = 2\r\nparams_xgb['max_depth']   = 6\r\nparams_xgb['eval_metric'] = 'mlogloss'\r\n\r\nnbr = 90\r\n\r\ny_hat_xgb = np.zeros(X_test.shape[0])\r\ny_oof_xgb = np.zeros(X_train.shape[0])\r\nfold      = 1\r\nkf      = KFold(n_splits = EPOCHS, shuffle = True, random_state=2019)\r\nkvs = '.'.join([str(k) + '=' + str(v).replace(':','') for k,v in zip(list(params_xgb), [str(value) for value in params_xgb.values()])])\r\nkvs += 'nbr=' + str(nbr)\r\nfor tr_idx, val_idx in kf.split(X_train, Y_train):\r\n    postfix = 'epochs=' + str(EPOCHS) + 'fold=' + str(fold) + kvs\r\n    filename = PATH + '/model_xgb.' + postfix + '.pkl'\r\n    X_tr, X_vl = X_train[tr_idx], X_train[val_idx, :]\r\n    y_tr, y_vl = Y_train[tr_idx], Y_train[val_idx]\r\n    if os.path.isfile(filename) and 1==1:\r\n        model_xgb = pickle.load(open(filename, 'rb'))\r\n    else:\r\n        model_xgb = xgb.train(params_xgb, xgb.DMatrix(X_tr, label=y_tr), num_boost_round=nbr)\r\n        s = pickle.dump(model_xgb, open(filename,'wb'))\r\n    y_pred_train = model_xgb.predict( xgb.DMatrix(X_vl, label=y_vl) )[:,1]\r\n    y_oof_xgb[val_idx] = y_pred_train\r\n    \r\n    y_zero = max(np.mean(y_vl), 1-np.mean(y_vl))\r\n    ACC    = accuracy_score(y_vl, (y_pred_train > 0.5  ).astype(int) )\r\n    AUC    = roc_auc_score( y_vl, y_pred_train)\r\n    LIFT   = ( ACC - y_zero )*100\r\n    print('  xgb: ', 'AUC:', AUC, 'ACC:', ACC, 'LIFT:', LIFT)\r\n\r\n    y_hat_xgb+= model_xgb.predict( xgb.DMatrix(X_test ))[:,1] / EPOCHS\r\n    fold+=1\r\n\r\nprint('  xgb AVG: ', 'AUC:', roc_auc_score( Y_train, y_oof_xgb), 'ACC:', accuracy_score(Y_train, (y_oof_xgb > 0.5  ).astype(int) ))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "d = model_xgb.get_fscore()\r\nl = [  d['f' + str(i)] if 'f' + str(i) in d else 0  for i in range( len(features_lr)) ]\r\nfeature_imp_xgb = pd.DataFrame(sorted(zip(l,features_lr)), columns=['Value','Feature'])\r\n\r\nplt.figure(figsize=(20, 10))\r\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp_xgb.sort_values(by=\"Value\", ascending=False))\r\nplt.title('XGB Features (avg over folds)')\r\nplt.tight_layout()\r\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_train['y_oof_xgb'] = y_oof_xgb\r\ndf_train[['ID','y_oof_xgb']].to_csv(PATH + 'y_hat_xgbw_train')\r\n\r\ndf_test[target]= y_hat_xgb\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.xgbw.' + postfix + '.csv', index = False, float_format = '%.4f')\r\nprint('  ', np.mean(y_hat_xgb))\r\n\r\nmodel_xgb = xgb.train(params_xgb, xgb.DMatrix(X_train, label=Y_train), num_boost_round=nbr)\r\ny_hat_xgb = model_xgb.predict( xgb.DMatrix(X_test))[:,1]\r\n\r\ndf_test[target]= y_hat_xgb\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.xgbw.' + 'full' + postfix + '.csv', index = False, float_format = '%.4f')\r\nprint('  ', np.mean(y_hat_xgb), params_xgb)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "d = model_xgb.get_fscore()\r\nl = [  d['f' + str(i)] if 'f' + str(i) in d else 0  for i in range( len(features_lr)) ]\r\nfeature_imp_xgb = pd.DataFrame(sorted(zip(l,features_lr)), columns=['Value','Feature'])\r\n\r\nplt.figure(figsize=(20, 10))\r\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp_xgb.sort_values(by=\"Value\", ascending=False))\r\nplt.title('XGB Feature Importance')\r\nplt.tight_layout()\r\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from sklearn.metrics import roc_curve\nfpr_xgb, tpr_xgb, _ = roc_curve(Y_train,  y_oof_xgb)\nplt.plot(fpr_xgb,tpr_xgb,label=\"XG Boost                , auc=\"+str(  round( roc_auc_score(Y_train, y_oof_xgb),4 )))\nfpr_lr, tpr_lr, _ = roc_curve(Y_train,  y_oof_lr)\nplt.plot(fpr_lr,tpr_lr,label=\"Logistic Regression, auc=\"+str(  round( roc_auc_score(Y_train, y_oof_lr),4 )))\nplt.legend(loc=4)\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "features_mpa = ['cand_assess_overall_score', 'commute_distance', 'shift_duration', 'shift_start_time_group'\r\n, 'sst', 'set', 'stt', 'appt_1_hour', 'appt_1_week', 'app_create_to_app_1'\r\n, 'work_days', 'appt_1_day_apts', 'appt_1_date_apts', 'app_esl_status_ESL', 'app_esl_status_NonESL', 'app_esl_status_ESLNAN'\r\n, 'shift_schedule_type_Other', 'shift_schedule_type_flex', 'shift_schedule_type_full', 'shift_schedule_type_part'\r\n, 'shift_schedule_type_reduced', 'app_id_1', 'app_id_2', 'app_id_3', 'app_id_4', 'app_id_5', 'app_id_6', 'app_id_7'\r\n, 'shift_startday_Fri', 'shift_startday_Mon', 'shift_startday_Sat', 'shift_startday_Sun', 'shift_startday_Thu', 'shift_startday_Tue', 'shift_startday_Wed', 'shift_startday_You'\r\n, 'events']\r\n \r\nX_train3 = preprocessing.scale(  np.nan_to_num( df[df.train >= 0][ features_mpa ].values )  )\r\nX_test3   = preprocessing.scale(  np.nan_to_num( df[df.train ==-1][ features_mpa ].values )  )" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## NB\r\n###############################################################################\r\nprint('\\n Naive Bayes...')\r\ny_hat_nb = np.zeros(X_test3.shape[0])\r\ny_oof_nb = np.zeros(X_train3.shape[0])\r\nkf      = KFold(n_splits = EPOCHS, shuffle = True, random_state=2019)\r\nfold      = 1\r\nfor tr_idx, val_idx in kf.split(X_train3, Y_train):\r\n    X_tr, X_vl = X_train3[tr_idx], X_train3[val_idx, :]\r\n    y_tr, y_vl = Y_train[tr_idx], Y_train[val_idx]\r\n    model_nb = GaussianNB().fit(X_tr, y_tr)\r\n \r\n    y_pred_train = model_nb.predict_proba(X_vl)[:,1]\r\n    y_oof_nb[val_idx] = y_pred_train\r\n    \r\n    y_zero = max(np.mean(y_vl), 1-np.mean(y_vl))\r\n    ACC    = accuracy_score(y_vl, (y_pred_train > 0.5  ).astype(int) )\r\n    AUC    = roc_auc_score( y_vl, y_pred_train)\r\n    LIFT   = ( ACC - y_zero )*100\r\n    print('  NB: ', 'AUC:', AUC, 'ACC:', ACC, 'LIFT:', LIFT)\r\n\r\n    y_hat_nb+= model_nb.predict_proba(X_test3)[:,1] / EPOCHS\r\n    fold+=1\r\n\r\nprint('  NB AVG: ', 'AUC:', roc_auc_score( Y_train, y_oof_nb), 'ACC:', accuracy_score(Y_train, (y_oof_nb > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  y_oof_nb > 0.5  ).astype(int)) )\r\n\r\ndf_test[target]= y_hat_nb\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.nb.' + 'folds' + str(EPOCHS) + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_nb))\r\n\r\nmodel_nb_full = GaussianNB().fit(X_train3, Y_train)\r\ny_hat_nb_full = model_nb_full.predict_proba(X_test3)[:,1]\r\n\r\ndf_test[target]= y_hat_nb_full\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.nb.' + 'full' + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_nb_full))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## MLP\r\n###############################################################################\r\n\r\nprint('\\n MLP...')\r\ny_hat_mlp = np.zeros(X_test3.shape[0])\r\ny_oof_mlp = np.zeros(X_train3.shape[0])\r\nkf      = KFold(n_splits = EPOCHS, shuffle = True, random_state=2019)\r\n\r\nfold      = 1\r\nfor tr_idx, val_idx in kf.split(X_train3, Y_train):\r\n    X_tr, X_vl = X_train3[tr_idx], X_train3[val_idx, :]\r\n    y_tr, y_vl = Y_train[tr_idx], Y_train[val_idx]\r\n    model_mlp = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(5, 2), random_state=2019).fit(X_tr, y_tr)\r\n    y_pred_train = model_mlp.predict_proba(X_vl)[:,1]\r\n    y_oof_mlp[val_idx] = y_pred_train\r\n    y_zero = max(np.mean(y_vl), 1-np.mean(y_vl))\r\n    ACC    = accuracy_score(y_vl, (y_pred_train > 0.5  ).astype(int) )\r\n    AUC    = roc_auc_score( y_vl, y_pred_train)\r\n    LIFT   = ( ACC - y_zero )*100\r\n    print('  MLP: ', 'AUC:', AUC, 'ACC:', ACC, 'LIFT:', LIFT)\r\n    y_hat_mlp+= model_mlp.predict_proba(X_test3)[:,1] / EPOCHS\r\n    fold+=1\r\n\r\nprint('  MLP AVG: ', 'AUC:', roc_auc_score( Y_train, y_oof_mlp), 'ACC:', accuracy_score(Y_train, (y_oof_mlp > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  y_oof_mlp > 0.5  ).astype(int)) )\r\n\r\ndf_test[target]= y_hat_mlp\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.mlp.' + 'folds' + str(EPOCHS) + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_mlp))\r\n\r\nmodel_mlp_full = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(5, 2), random_state=2019).fit(X_train3, Y_train)\r\ny_hat_mlp_full = model_mlp_full.predict_proba(X_test3)[:,1]\r\n\r\ndf_test[target]= y_hat_mlp_full\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.mlp.' + 'full' + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_mlp_full))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## MLP ADAM\r\n###############################################################################\r\n\r\nparams_mlp = {}\r\nparams_mlp['random_state']       = 2019\r\nparams_mlp['max_iter']           = 200\r\nparams_mlp['hidden_layer_sizes'] = (5,2)\r\nparams_mlp['alpha']              = 0.00025\r\nparams_mlp['solver']             = 'adam'\r\nparams_mlp['epsilon']            = 2e-4\r\nparams_mlp['activation']         = 'relu'\r\nparams_mlp['beta_1']             = .91\r\nparams_mlp['beta_2']             = .99\r\n\r\n\r\nprint('\\n  MLP ADAM...')\r\ny_hat_mlpa = np.zeros(X_test3.shape[0])\r\ny_oof_mlpa = np.zeros(X_train3.shape[0])\r\nkf      = KFold(n_splits = EPOCHS, shuffle = True, random_state=2019)\r\nfold      = 1\r\nfor tr_idx, val_idx in kf.split(X_train3, Y_train):\r\n    X_tr, X_vl = X_train3[tr_idx], X_train3[val_idx, :]\r\n    y_tr, y_vl = Y_train[tr_idx], Y_train[val_idx]\r\n    model_mlpa = MLPClassifier(**params_mlp).fit(X_tr, y_tr)\r\n    y_pred_train = model_mlpa.predict_proba(X_vl)[:,1]\r\n    y_oof_mlpa[val_idx] = y_pred_train\r\n    y_zero = max(np.mean(y_vl), 1-np.mean(y_vl))\r\n    ACC    = accuracy_score(y_vl, (y_pred_train > 0.5  ).astype(int) )\r\n    AUC    = roc_auc_score( y_vl, y_pred_train)\r\n    LIFT   = ( ACC - y_zero )*100\r\n    print('  MLP ADAM: ', 'AUC:', AUC, 'ACC:', ACC, 'LIFT:', LIFT)\r\n    y_hat_mlpa+= model_mlpa.predict_proba(X_test3)[:,1] / EPOCHS\r\n    fold+=1\r\n\r\nprint('  MLP AVG ADAM: ', 'AUC:', roc_auc_score( Y_train, y_oof_mlpa), 'ACC:', accuracy_score(Y_train, (y_oof_mlpa > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  y_oof_mlpa > 0.5  ).astype(int)) )\r\n\r\ndf_test[target]= y_hat_mlpa\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.mlpa.' + 'folds' + str(EPOCHS) + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_mlpa))\r\n\r\nmodel_mlpa_full = MLPClassifier(**params_mlp).fit(X_train3, Y_train)\r\ny_hat_mlpa_full = model_mlpa_full.predict_proba(X_test3)[:,1]\r\n\r\ndf_test[target]= y_hat_mlpa_full\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.mlpa.' + 'full' + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_mlpa_full))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## LGB\r\n###############################################################################\r\nparams_lgb['lambda']          = 0.10\r\nparams_lgb['max_depth']       = 3\r\n\r\nprint('\\n lgbboost...')\r\ny_hat_lgb = np.zeros(X_test.shape[0])\r\ny_oof_lgb = np.zeros(X_train.shape[0])\r\nfold      = 1\r\nkf      = KFold(n_splits = EPOCHS, shuffle = True, random_state=2019)\r\nkvs = '.'.join([str(k) + '=' + str(v).replace(':','') for k,v in zip(list(params_lgb), [str(value) for value in params_lgb.values()])])\r\nnbr_lgb = 300\r\n\r\nfor tr_idx, val_idx in kf.split(X_train, Y_train):\r\n    postfix = 'epochs=' + str(EPOCHS) + 'fold=' + str(fold) + kvs\r\n    filename = PATH + '/pickles/model_lgb.' + postfix + '.pkl'\r\n    X_tr, X_vl = X_train[tr_idx], X_train[val_idx, :]\r\n    y_tr, y_vl = Y_train[tr_idx], Y_train[val_idx]\r\n    if os.path.isfile(filename):\r\n        model_lgb = pickle.load(open(filename, 'rb'))\r\n    else:\r\n        model_lgb = lgb.train(params_lgb, lgb.Dataset(X_tr, label = y_tr), num_boost_round=nbr_lgb)\r\n        s = pickle.dump(model_lgb, open(filename,'wb'))\r\n    y_pred_train = model_lgb.predict(X_vl)[:,1]\r\n    y_oof_lgb[val_idx] = y_pred_train\r\n    \r\n    y_zero = max(np.mean(y_vl), 1-np.mean(y_vl))\r\n    ACC    = accuracy_score(y_vl, (y_pred_train > 0.5  ).astype(int) )\r\n    AUC    = roc_auc_score( y_vl, y_pred_train)\r\n    LIFT   = ( ACC - y_zero )*100\r\n    print('  lgb: ', 'AUC:', AUC, 'ACC:', ACC, 'LIFT:', LIFT)\r\n\r\n    y_hat_lgb+= model_lgb.predict(X_test)[:,1] / EPOCHS\r\n    fold+=1\r\n\r\nprint('  lgb AVG: ', 'AUC:', roc_auc_score( Y_train, y_oof_lgb), 'ACC:', accuracy_score(Y_train, (y_oof_lgb > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  y_oof_lgb > 0.5  ).astype(int)) )\r\n\r\ndf_test[target]= y_hat_lgb\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.lgb.' + postfix + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_lgb))\r\n\r\nmodel_lgb_full = lgb.train(params_lgb, lgb.Dataset(X_train, label = Y_train), num_boost_round=nbr_lgb)\r\ny_hat_lgb_full = model_lgb_full.predict(X_test)[:,1]\r\n\r\n\r\ndf_test[target]= y_hat_lgb_full\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.lgb.' + 'full' + '.csv', index = False, float_format = '%.4f')\r\n\r\n\r\nprint('  ', np.mean(y_hat_lgb_full))\r\n\r\nfeature_imp = pd.DataFrame(sorted(zip(model_lgb.feature_importance(),features_lr)), columns=['Value','Feature'])\r\nplt.figure(figsize=(20, 10))\r\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\r\nplt.title('LightGBM Features (avg over folds)')\r\nplt.tight_layout()\r\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "###############################################################################\r\n## CatBoost\r\n###############################################################################\r\nprint(' Catboost...')\r\nparams_cat = {}\r\nparams_cat['loss_function'] = 'MultiClass'\r\nparams_cat['random_seed'] =   2019\r\nparams_cat['classes_count'] = 2\r\nparams_cat['l2_leaf_reg']   = 3\r\nparams_cat['depth']         = 6\r\nparams_cat['learning_rate'] = 0.043\r\nparams_cat['iterations']    = 250\r\nparams_cat['verbose'] = False\r\n\r\ny_hat_cat = np.zeros(X_test.shape[0])\r\ny_oof_cat = np.zeros(X_train.shape[0])\r\nfold      = 1\r\nkf      = KFold(n_splits = EPOCHS, shuffle = True, random_state=2019)\r\n\r\nkvs = '.'.join([str(k) + '=' + str(v).replace(':','') for k,v in zip(list(params_cat), [str(value) for value in params_cat.values()])])\r\n\r\nfor tr_idx, val_idx in kf.split(X_train, Y_train):\r\n    postfix = 'epochs=' + str(EPOCHS) + 'fold=' + str(fold) + kvs\r\n    filename = PATH + '/pickles/model_cat.' + postfix + '.pkl'\r\n    X_tr, X_vl = X_train[tr_idx], X_train[val_idx, :]\r\n    y_tr, y_vl = Y_train[tr_idx], Y_train[val_idx]\r\n    if os.path.isfile(filename):\r\n        model_cat = pickle.load(open(filename, 'rb'))\r\n    else:\r\n        model_cat = CatBoostClassifier(**params_cat).fit(X_tr, y_tr)\r\n        s = pickle.dump(model_cat, open(filename,'wb'))\r\n    y_pred_train = model_cat.predict_proba(X_vl)[:,1]\r\n    y_oof_cat[val_idx] = y_pred_train\r\n    \r\n    y_zero = max(np.mean(y_vl), 1-np.mean(y_vl))\r\n    ACC    = accuracy_score(y_vl, (y_pred_train > 0.5  ).astype(int) )\r\n    AUC    = roc_auc_score( y_vl, y_pred_train)\r\n    LIFT   = ( ACC - y_zero )*100\r\n    print('  CAT: ', 'AUC:', AUC, 'ACC:', ACC, 'LIFT:', LIFT)\r\n\r\n    y_hat_cat+= model_cat.predict_proba(X_test)[:,1] / EPOCHS\r\n    fold+=1\r\n\r\nprint('  CAT AVG: ', 'AUC:', roc_auc_score( Y_train, y_oof_cat), 'ACC:', accuracy_score(Y_train, (y_oof_cat > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  y_oof_cat > 0.5  ).astype(int)) )\r\n\r\ndf_test[target]= y_hat_cat\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.cat.' + postfix + '.csv', index = False, float_format = '%.4f')\r\n\r\nprint('  ', np.mean(y_hat_cat))\r\n\r\nmodel_cat_full = CatBoostClassifier(**params_cat).fit(X_train, Y_train)\r\ny_hat_cat_full = model_cat_full.predict_proba(X_test)[:,1]\r\n\r\ndf_test[target]= y_hat_cat_full\r\ndf_test[['ID',target]].to_csv(PATH + 'sub.cat.' + 'full' + '.csv', index = False, float_format = '%.4f')\r\n\r\n\r\nprint('  ', np.mean(y_hat_cat_full))\r\n\r\nfeature_imp = pd.DataFrame(sorted(zip(model_cat.get_feature_importance(),features_lr)), columns=['Value','Feature'])\r\nplt.figure(figsize=(20, 10))\r\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\r\nplt.title('LightGBM Features (avg over folds)')\r\nplt.tight_layout()\r\nplt.show()\r\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "y_hat_ens_oof = (y_oof_cat**2      + y_oof_xgb**4 + y_oof_mlp**4       + y_oof_mlpa**4      )/(4)\r\ny_hat_ens     = (y_hat_cat_full**2 + y_hat_xgb**4 + y_hat_mlp_full**4  + y_hat_mlpa_full**4 )/(4)\r\nprint('  ENS AVG: ', 'AUC:', roc_auc_score( Y_train, y_hat_ens_oof), 'ACC:', accuracy_score(Y_train, (y_hat_ens_oof > 0.5  ).astype(int) ))\r\nprint( confusion_matrix(Y_train, (  y_hat_ens_oof > 0.2  ).astype(int)) )\r\nprint('ENS: ', np.mean(y_hat_ens))\r\n\r\nnarf = df_train.cand_id.value_counts().reset_index()\r\nnarf.columns = ['cand_id', 'events']\r\nv1 = np.mean(df_train[df_train.cand_id.isin( narf[narf.events==1].cand_id   )][target])\r\nv2 = np.mean(df_train[df_train.cand_id.isin( narf[narf.events>1].cand_id   )][target])\r\n\r\nnarf = pd.concat(  [ df_train[['ID', 'cand_id']], df_test[['ID', 'cand_id']] ], axis=0 ).cand_id.value_counts().reset_index()\r\nnarf.columns = ['cand_id', 'events']\r\n\r\ndf_test['narf'] = v1\r\ndf_test.loc[ df_test.cand_id.isin( narf[narf.events>1].cand_id)  , 'narf'] = v2\r\n\r\n###############################################################################\r\n## Stacking\r\n###############################################################################\r\ndf_train['y_hat_lr']    = y_oof_lr**2\r\ndf_train['y_hat_nb']    = y_oof_nb**2\r\ndf_train['y_hat_mlp']   = y_oof_mlp**2\r\ndf_train['y_hat_mlpa']  = y_oof_mlpa**2\r\ndf_train['y_hat_lgb']   = y_oof_lgb**2\r\ndf_train['y_hat_cat']   = y_oof_cat\r\ndf_train['y_hat_xgb']   = y_oof_xgb**2\r\n\r\ndf_test['y_hat_lr']    = y_hat_lr_full**2\r\ndf_test['y_hat_nb']    = y_hat_nb_full**2\r\ndf_test['y_hat_mlp']   = y_hat_mlp_full**2\r\ndf_test['y_hat_mlpa']  = y_hat_mlpa_full**2\r\ndf_test['y_hat_lgb']   = y_hat_lgb_full**2\r\ndf_test['y_hat_cat']   = y_hat_cat\r\ndf_test['y_hat_xgb']   = y_hat_xgb**2\r\n\r\ncols = ['y_hat_lr', 'y_hat_mlp', 'y_hat_mlpa', 'y_hat_cat', 'y_hat_xgb']\r\nx_train = df_train[cols]\r\nx_test  = df_test[cols]\r\n\r\nmodel_rf = RandomForestClassifier(n_estimators=300, max_depth = 4, random_state = 2019).fit( x_train, Y_train)\r\ny_hat_rf = model_rf.predict_proba( x_test )[:,1]\r\n\r\n###############################################################################\r\n## Write Outputs\r\n###############################################################################\r\ndf_test[target]= y_hat_rf\r\ndf_test[['ID',target]].to_csv(PATH + 'submission.stack.{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index = False, float_format = '%.4f')\r\n\r\n\r\ndf_test[target]= y_hat_ens\r\ndf_test[['ID',target]].to_csv(PATH + 'submission.ens.{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index = False, float_format = '%.4f')\r\n\r\ndf_test[target]= df_test.narf *0.1 + y_hat_ens *0.9\r\nprint(np.mean(df_test[target]))\r\ndf_test[['ID',target]].to_csv(PATH + 'submission.narf.{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index = False, float_format = '%.4f')\r\n\r\n## Save Python To File\r\nos.system(\"copy \" + PATH.replace('/','\\\\') + \"wfs_fin.py \" + PATH.replace('/','\\\\') + \"wfs_fin.py.{}\".format(datetime.now().strftime('%Y%m%d_%H%M%S')))\r\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "for i in [1,2,4,8,16,32,64]:\r\n    y_hat_ens_pow = (y_oof_xgb**i + y_oof_mlp**i + y_oof_mlpa**i)/(34)\r\n    fpr, tpr, _ = roc_curve(Y_train,  y_hat_ens_pow)\r\n    auc = roc_auc_score(Y_train, y_hat_ens_pow)\r\n    plt.plot(fpr,tpr,label='Power ' + str(i) + ', auc='+str(round( auc, 4)))\r\n    plt.legend(loc=4)" ]
  } ]
}