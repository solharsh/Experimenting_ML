{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach overview\n",
    "-  Build Ensemble that includes multiple model categories: Logistic Regression, Random Forests, XGBoost, Adaboost, and Neural Networks.\n",
    "-  Split the training dataset into K stratified folds. For each fold and model category, train a separate model using Grid Search.\n",
    "-  Combine all models into ensemble using Averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We experimented with: \n",
    "1. Which model categories to include in the ensemble  \n",
    "2. How many stratified folds to use: 1, 5, 10, 20, 40 \n",
    "3. How to build the ensemble: Averaging vs. Max voting\n",
    "4. Oversampling techniques such as SMOTE and ADASYN: including models trained with SMOTE data in the ensemble worked for the Public leaderboad, but not for Private\n",
    "5. Feature standardization: did not seem to improve anything.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "-  Ensembling is the way to go, of course.\n",
    "-  Increasing the number of stratified folds improved performance.\n",
    "-  Improvements in training data ccuracy (on validation set) did not necessarrily translate to better accuracies in the Public dataset. A prime example for this was the LR method that did not perform as well in the training validation accuracy compared to other methods such as NN. However, LR was an integral part of the overall Ensemble; whenever we removed it, the Public dataset accuracy ended up much worse.\n",
    "-  Ensembling using Averaging always worked better than Max voting.\n",
    "-  We kind of `overfitted' to the Public Leaderboard, i.e., our best performing model in Public was not the best in Private. \n",
    "-  Adding models trained with oversampled data, using either SMOTE or ADASYN, decreased accuracy in Private dataset. \n",
    "-  Gini impurity appeared to work better than Entropy for tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Mar 26 14:35:22 2019\n",
    "\n",
    "@author: nikosc\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, numpy as np, time, sys, h5py\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from keras.layers import Input, Dense , Dropout , TimeDistributed , LSTM , GRU, concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD , Adadelta, RMSprop, Adam, Adamax\n",
    "from keras.models import  load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import  to_categorical \n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Initialize problem parameters\n",
    "class Args:\n",
    "    \"\"\" Class containing all model arguments \"\"\"\n",
    "    def __init__( self ):\n",
    "        self.project    = 'MLchallenge_DontOverfit'\n",
    "        self.dataPath   = '/home/ubuntu/{}/'       .format(self.project)\n",
    "        self.modelsPath = '/home/ubuntu/{}/Models' .format(self.project)\n",
    "        self.resultsPath= '/home/ubuntu/{}/Results'.format(self.project)\n",
    "        self.CV_folds   = 40  # split the Training data in stratified folds, to train different versions of models \n",
    "args = Args()\n",
    "##########################################\n",
    "\n",
    "\n",
    "########################################## \n",
    "############\n",
    "# LOAD DATA\n",
    "train = pd.read_csv( args.dataPath + 'TTT_train.csv' )\n",
    "test  = pd.read_csv( args.dataPath + 'TTT_test_features.csv',index_col = 'ID')\n",
    "#train.describe()\n",
    "X = train.loc[:, train.columns != 'label']\n",
    "y = train['label']\n",
    "y_cat = to_categorical(y)\n",
    "# Generate a set of stratified folds of Training to train different versions of each model.\n",
    "folds = list(StratifiedKFold(n_splits=args.CV_folds, shuffle=True, random_state=1).split(X, y))\n",
    "############\n",
    "###########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some functions for model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# function to fit a model on every fold, and store trained model\n",
    "def fitValidateSave( model, modelType ):\n",
    "    #\n",
    "    accuracies = []\n",
    "    for foldIndex, fold in enumerate(folds):\n",
    "        X_fold      = np.take( X, fold[0], axis=0)\n",
    "        y_fold      = np.take( y, fold[0], axis=0)\n",
    "        #\n",
    "        #oversampler = RandomOverSampler(random_state=77)\n",
    "        #X_fold, y_fold = oversampler.fit_sample(X_fold, y_fold)\n",
    "        #\n",
    "        X_fold_test = np.take( X, fold[1], axis=0)\n",
    "        y_fold_test = np.take( y, fold[1], axis=0)\n",
    "        #\n",
    "        model.fit(X_fold, y_fold)\n",
    "        #\n",
    "        accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
    "        print( '{}: {}'.format(foldIndex, accuracies[-1]) )\n",
    "        print(model.best_params_)\n",
    "        #\n",
    "        pickle.dump( model, open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldIndex ) , 'wb'))\n",
    "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
    "    return model\n",
    "##################################################\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Compute accuracies across folds using an already trained model.\n",
    "def validateAcrossFolds( modelType ):\n",
    "    #\n",
    "    accuracies = []\n",
    "    for foldInd, fold in enumerate(folds):\n",
    "        X_fold_test = np.take( X, fold[1], axis=0)\n",
    "        y_fold_test = np.take( y, fold[1], axis=0)\n",
    "        #\n",
    "        if 'NN' in modelType:\n",
    "            y_fold_test = to_categorical(y_fold_test)\n",
    "            model = load_model( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ) )\n",
    "            accuracies.append( model.evaluate(X_fold_test, y_fold_test, batch_size=512, verbose=0 )[1] )\n",
    "        else:\n",
    "            model = pickle.load(open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ), 'rb'))\n",
    "            accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
    "        print( '{}: {}'.format(foldInd, accuracies[-1]) )\n",
    "        #\n",
    "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
    "    return model\n",
    "##################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"penalty\":[\"l2\"],\n",
    "    \"C\": [ 3., 4., 5.],\n",
    "    \"fit_intercept\": [True],\n",
    "    \"class_weight\":['balanced'],\n",
    "    \"solver\":[ 'lbfgs' ],\n",
    "    \"multi_class\": [\"multinomial\"],\n",
    "    \"random_state\":[77]\n",
    "    }\n",
    "LR = GridSearchCV(LogisticRegression(), \n",
    "                  parameters, \n",
    "                  cv=4, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "LR = fitValidateSave( LR, 'LR' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"criterion\":[\"gini\"],\n",
    "    \"max_depth\":[ 15, 30  ],\n",
    "    \"min_samples_split\": [ 5 ],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"max_features\":[None ],\n",
    "    \"random_state\": [77],\n",
    "    \"n_estimators\":[ 200 ]\n",
    "    }\n",
    "RF_gini = GridSearchCV(RandomForestClassifier(), \n",
    "                  parameters, \n",
    "                  cv=4, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "RF_gini = fitValidateSave( RF_gini, 'RF_gini' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_gini = AdaBoostClassifier( base_estimator = DecisionTreeClassifier( \n",
    "                             criterion         = 'gini', \n",
    "                             splitter          = 'random',\n",
    "                             max_depth         = 30, \n",
    "                             min_samples_split = 5, \n",
    "                             min_samples_leaf  = 1,\n",
    "                             max_features      = None,\n",
    "                             random_state      = 77 \n",
    "                            ),\n",
    "                            learning_rate= 1,\n",
    "                            n_estimators = 200\n",
    "                         )\n",
    "AB_gini = fitValidateAndSave( AB_gini, 'AB_gini' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(  max_depth=6,  \n",
    "                      learning_rate=0.1, \n",
    "                      n_estimators=100, \n",
    "                      verbosity=1, \n",
    "                      objective='multi:softmax', \n",
    "                      num_class=y_cat.shape[-1],\n",
    "                      booster='gbtree', \n",
    "                      n_jobs=4, \n",
    "                      gamma=0, \n",
    "                      min_child_weight=1,\n",
    "                      max_delta_step=0, \n",
    "                      subsample=.7, \n",
    "                      colsample_bytree=.6, \n",
    "                      colsample_bylevel=.6, \n",
    "                      colsample_bynode=.6, \n",
    "                      reg_alpha=.0, \n",
    "                      reg_lambda=.0, \n",
    "                      scale_pos_weight=1, \n",
    "                      base_score=0.1, \n",
    "                      random_state=77 \n",
    "                      )\n",
    "XGB = fitValidateAndSave( XGB, 'XGB' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "def saveToH5( data , filePath , fillvalue=0 ):\n",
    "    h5f = h5py.File( filePath , 'w')\n",
    "    h5f.create_dataset('dataset', data =  data ,fillvalue=fillvalue ,compression='gzip', compression_opts=4 ); \n",
    "    h5f.close()\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "def loadFromH5( filePath ):\n",
    "    h5f = h5py.File( filePath , 'r')\n",
    "    output =   h5f['dataset'][:]  ; h5f.close()\n",
    "    return output\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "def buildMLP():\n",
    "    # DEFINE MLP MODEL\n",
    "    main_input = Input( shape=( X.shape[-1], ) ,  name = 'features' )\n",
    "    x = Dropout(0.8) (main_input)\n",
    "    #x = BatchNormalization(axis = -1)(main_input)\n",
    "    x = Dense( nodes, activation='relu',\n",
    "                           kernel_regularizer   =reg,\n",
    "                           activity_regularizer =reg,\n",
    "                           bias_regularizer     =reg\n",
    "              )(x)\n",
    "    x = Dropout( drops ) (x)\n",
    "    \n",
    "    ###\n",
    "    for lay in range(layers-1):\n",
    "        #\n",
    "        if True:\n",
    "            x = concatenate([x, main_input])\n",
    "        #\n",
    "        #x = BatchNormalization(axis = -1)(x)\n",
    "        x = Dense( nodes, activation='relu' )(x)\n",
    "        x = Dropout( drops ) (x)\n",
    "    ###       \n",
    "    output =  Dense( y_cat.shape[-1], activation='softmax', name = 'output' )(x)     \n",
    "    \n",
    "    ###\n",
    "    model = Model(input=main_input, output=output)\n",
    "    ###\n",
    "    model.compile( optimizer=optimizer , \n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['categorical_accuracy']\n",
    "                   )\n",
    "    ###\n",
    "    #model.summary()  \n",
    "    return model\n",
    "############\n",
    "\n",
    "############\n",
    "# TRAIN MODELS\n",
    "def trainMLP(modelName='NN'):\n",
    "    accuracies = []\n",
    "    for foldInd, fold in enumerate(folds[:]):\n",
    "        model = buildMLP()\n",
    "        X_fold = np.take( X, fold[0], axis=0)\n",
    "        y_fold = np.take( y_cat, fold[0], axis=0)\n",
    "        X_fold_test = np.take( X, fold[1], axis=0)\n",
    "        y_fold_test = np.take( y_cat, fold[1], axis=0)\n",
    "        \n",
    "        \n",
    "        loss_history =[] ;  no_improvement = 0 ; break_it = 0\n",
    "        for epok in range( 0 , 1000 ) :\n",
    "            if break_it :\n",
    "                break\n",
    "            \n",
    "            model.fit(X_fold, \n",
    "                      y_fold, \n",
    "                      batch_size=batchSize,\n",
    "                      shuffle=True,\n",
    "                      epochs=1, \n",
    "                      verbose=0,\n",
    "                      validation_data=(X_fold_test, y_fold_test)\n",
    "                      )\n",
    "            \n",
    "            \n",
    "            loss_history.append( model.evaluate( x=X_fold_test, y=y_fold_test, batch_size=512, verbose=2)[1] )\n",
    "            #\n",
    "            if len(loss_history)>1:\n",
    "                if loss_history[-1] <= max( loss_history[:-1] ):\n",
    "                    no_improvement +=1\n",
    "                else:\n",
    "                    no_improvement = 0\n",
    "                    model.save( '{}/{}_fold{}.h5'.format( args.modelsPath, modelName, foldInd )  )\n",
    "                #                            \n",
    "                if no_improvement >= 20:\n",
    "                    break_it = 1\n",
    "                    accuracies.append( max(loss_history) )\n",
    "                    print( '{}: {}'.format( foldInd, accuracies[-1] ) )\n",
    "                    break\n",
    "    print( 'Average accuracy for {} is:  {}'.format( 'NN', np.mean(accuracies)) )  \n",
    "################################################\n",
    "\n",
    "\n",
    "###########\n",
    "index_ = 0 \n",
    "#\n",
    "layers = 2\n",
    "nodes = 512\n",
    "drops = 0.5\n",
    "batchSize = 128\n",
    "lr=0.003\n",
    "optimizer = Adam(lr=lr)\n",
    "reg=l1_l2(l1=0.0001, l2=0.0005)\n",
    "#\n",
    "trainMLP( modelName='NN{}'.format(index_) )\n",
    "###########\n",
    "\n",
    "\n",
    "###########\n",
    "index_ = 1 \n",
    "#\n",
    "layers = 2\n",
    "nodes = 512\n",
    "drops = 0.5\n",
    "batchSize = 128\n",
    "lr=0.003\n",
    "optimizer = Adam(lr=lr)\n",
    "reg=l1_l2(l1=0.0001, l2=0.001)\n",
    "#\n",
    "trainMLP( modelName='NN{}'.format(index_) )\n",
    "###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more functions to generate the Ensemble prediction on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "def genTestPredictionsPerModelInstance( modelInstance ):\n",
    "    # Generate predictions over test set\n",
    "    predictions = np.zeros(( testData.shape[0], y_cat.shape[-1] ))\n",
    "    for foldIndex, fold in enumerate(folds[:]):\n",
    "        print(foldIndex)\n",
    "        if 'NN' in modelInstance:\n",
    "            model = load_model( '{}/{}_fold{}.h5'.format( args.modelsPath, modelInstance, foldIndex ) )\n",
    "            predictions += model.predict(testData, batch_size=1024) \n",
    "        else:\n",
    "            model = pickle.load(open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelInstance, foldIndex ), 'rb'))\n",
    "            predictions += model.predict_proba(testData)\n",
    "    #\n",
    "    predictionsPath = '{}/predictions_{}.h5'.format( args.resultsPath, modelInstance )\n",
    "    saveToH5( predictions, predictionsPath ) \n",
    "    return model\n",
    "##################################################\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Get predictions \n",
    "def generateEnsemblePredictions(  ensemble = [ 'LR', 'RF_gini', 'AB_gini', 'NN', 'GB' ],                   \n",
    "                                  mode='sum'\n",
    "                                ): \n",
    "    #                      \n",
    "    ensemblePredictions = np.zeros(( testData.shape[0], len(ensemble), y_cat.shape[-1] ))\n",
    "    for modelInstanceIndex, modelInstance in enumerate(ensemble) :\n",
    "        print(modelInstance)\n",
    "        predictionsPath  = '{}/predictions_{}.h5'.format( args.resultsPath, modelInstance ) \n",
    "        modelPredictions = loadFromH5(predictionsPath)\n",
    "        ensemblePredictions[:,modelInstanceIndex] += np.copy( modelPredictions ) \n",
    "    #\n",
    "    if mode=='sum':\n",
    "        classPredictions = np.sum(ensemblePredictions, axis=1)   \n",
    "    elif mode=='max':\n",
    "        classPredictions = np.max(ensemblePredictions, axis=1)   \n",
    "    \n",
    "    #\n",
    "    classPredictions = np.argmax( classPredictions, axis=1 )      \n",
    "    predictions = testData.copy()\n",
    "    predictions['predictions'] = classPredictions \n",
    "    predictions = predictions['predictions']\n",
    "    predictions.to_csv( '{}/v0.1'.format(args.resultsPath ), index_label='ID', header=['label']   )\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ensemble predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Generate test predictions per modelInstance\n",
    "for modelInstance in [ 'LR', 'RF_gini', 'AB_gini', 'NN0', 'NN1'  ]:\n",
    "    print(modelInstance)\n",
    "    genTestPredictionsPerModelInstance( modelInstance ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Ensemble Predictions\n",
    "generateEnsemblePredictions( ensemble = [ 'LR', 'RF_gini', 'AB_gini', 'NN0', 'NN1' ] \n",
    "                           )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
