{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dont_Overfit.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/solharsh/Experimenting_ML/blob/master/Dont_Overfit.ipynb",
      "authorship_tag": "ABX9TyPrVVFXkKUPoQiasf9X67SW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solharsh/Experimenting_ML/blob/master/Dont_Overfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM4EAUsKjKwV",
        "colab_type": "text"
      },
      "source": [
        "# Avoid overfitting with a tiny sliver for training data\n",
        "Inspired by the Kaggle Don’t Overfit Challenge: Tiny Training Trial. The challenge; build the best performing model you can with a <5% training vs >95% test split with TF-IDF encodings on an Amazon multi-classification problem. With so many data hungry algorithms out there that take days or more to compute, we thought it’d be refreshing to go the other way and experiment with what can be done with extremely small and noisy datasets! Iterate and experiment with training times on the order of seconds. Our split is:\n",
        "\n",
        "Train: 1244 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B9uiNyojZSi",
        "colab_type": "text"
      },
      "source": [
        "Approach overview\n",
        "\n",
        "•Build Ensemble that includes multiple model categories: Logistic Regression, Random Forests, XGBoost, Adaboost, and Neural Networks.\n",
        "\n",
        "•Split the training dataset into K stratified folds. For each fold and model category, train a separate model using Grid Search.\n",
        "\n",
        "•Combine all models into ensemble using Averaging.\n",
        "\n",
        "I experimented with:\n",
        "\n",
        "- 1.Which model categories to include in the ensemble \n",
        "- 2.How many stratified folds to use: 1, 5, 10, 20, 40 \n",
        "- 3.How to build the ensemble: Averaging vs. Max voting\n",
        "- 4.Oversampling techniques such as SMOTE and ADASYN: including models trained with SMOTE data in the ensemble worked for the Public leaderboad, but not for Private\n",
        "- 5.Feature standardization: did not seem to improve anything.\n",
        "\n",
        "## Lessons Learned\n",
        "\n",
        "-  Ensembling is the way to go, of course.\n",
        "-  Increasing the number of stratified folds improved performance.\n",
        "-  Improvements in training data accuracy (on validation set) did not necessarrily translate to better accuracies in the Public dataset. A prime example for this was the LR method that did not perform as well in the training validation accuracy compared to other methods such as NN. However, LR was an integral part of the overall Ensemble; whenever we removed it, the Public dataset accuracy ended up much worse.\n",
        "-  Ensembling using Averaging always worked better than Max voting.\n",
        "-  We kind of `overfitted' to the Public Leaderboard, i.e., our best performing model in Public was not the best in Private. \n",
        "-  Adding models trained with oversampled data, using either SMOTE or ADASYN, decreased accuracy in Private dataset. \n",
        "-  Gini impurity appeared to work better than Entropy for tree-based models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HoYIusMjLSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting more RAM from google colab\n",
        "#a = []\n",
        "#while(1):\n",
        "#    a.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMCa6x2bjUnY",
        "colab_type": "code",
        "outputId": "eb7820ed-7727-4598-cadf-53a73e97a023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import pandas as pd, numpy as np, time, sys, h5py\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from keras.layers import Input, Dense , Dropout , TimeDistributed , LSTM , GRU, concatenate, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD , Adadelta, RMSprop, Adam, Adamax\n",
        "from keras.models import  load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import  to_categorical \n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import pickle\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmw7IXURjhhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize problem parameters\n",
        "class Args:\n",
        "    \"\"\" Class containing all model arguments \"\"\"\n",
        "    def __init__( self ):\n",
        "        self.project    = 'MLchallenge_DontOverfit'\n",
        "        self.dataPath   = '/content/drive/My Drive/Dont_Overfit/'       .format(self.project)\n",
        "        self.modelsPath = '/content/drive/My Drive/Dont_Overfit/' .format(self.project)\n",
        "        self.resultsPath= '/content/drive/My Drive/Dont_Overfit/'.format(self.project)\n",
        "        self.CV_folds   = 40  # split the Training data in stratified folds, to train different versions of models \n",
        "args = Args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx2H4OU9jywf",
        "colab_type": "code",
        "outputId": "bf3d5b07-4cba-455b-87c7-7101f8ea2442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# LOAD DATA\n",
        "train = pd.read_csv( args.dataPath + 'TTT_train.csv' )\n",
        "test  = pd.read_csv( args.dataPath + 'TTT_test_features.csv',index_col = 'ID')\n",
        "print(train.describe())\n",
        "X = train.loc[:, train.columns != 'label']\n",
        "y = train['label']\n",
        "y_cat = to_categorical(y)\n",
        "# Generate a set of stratified folds of Training to train different versions of each model.\n",
        "folds = list(StratifiedKFold(n_splits=args.CV_folds, shuffle=True, random_state=1).split(X, y))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                f0           f1  ...        f1255        label\n",
            "count  1244.000000  1244.000000  ...  1244.000000  1244.000000\n",
            "mean      0.000566     0.000697  ...     0.000496     5.167203\n",
            "std       0.019962     0.024577  ...     0.017501     3.662910\n",
            "min       0.000000     0.000000  ...     0.000000     0.000000\n",
            "25%       0.000000     0.000000  ...     0.000000     0.000000\n",
            "50%       0.000000     0.000000  ...     0.000000     6.000000\n",
            "75%       0.000000     0.000000  ...     0.000000     9.000000\n",
            "max       0.704060     0.866833  ...     0.617260     9.000000\n",
            "\n",
            "[8 rows x 1257 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 36 members, which is less than n_splits=40.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO9GSMzCj-Nc",
        "colab_type": "text"
      },
      "source": [
        "Some functions for model training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDLN1g4Hj-sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################\n",
        "# function to fit a model on every fold, and store trained model\n",
        "def fitValidateSave( model, modelType ):\n",
        "    #\n",
        "    accuracies = []\n",
        "    for foldIndex, fold in enumerate(folds):\n",
        "        X_fold      = np.take( X, fold[0], axis=0)\n",
        "        y_fold      = np.take( y, fold[0], axis=0)\n",
        "        #\n",
        "        #oversampler = RandomOverSampler(random_state=77)\n",
        "        #X_fold, y_fold = oversampler.fit_sample(X_fold, y_fold)\n",
        "        #\n",
        "        X_fold_test = np.take( X, fold[1], axis=0)\n",
        "        y_fold_test = np.take( y, fold[1], axis=0)\n",
        "        #\n",
        "        model.fit(X_fold, y_fold)\n",
        "        #\n",
        "        accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
        "        print( '{}: {}'.format(foldIndex, accuracies[-1]) )\n",
        "        #print(model.best_params_)\n",
        "        #\n",
        "        pickle.dump( model, open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldIndex ) , 'wb'))\n",
        "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
        "    return model\n",
        "##################################################\n",
        "\n",
        "\n",
        "##################################################\n",
        "# Compute accuracies across folds using an already trained model.\n",
        "def validateAcrossFolds( modelType ):\n",
        "    #\n",
        "    accuracies = []\n",
        "    for foldInd, fold in enumerate(folds):\n",
        "        X_fold_test = np.take( X, fold[1], axis=0)\n",
        "        y_fold_test = np.take( y, fold[1], axis=0)\n",
        "        #\n",
        "        if 'NN' in modelType:\n",
        "            y_fold_test = to_categorical(y_fold_test)\n",
        "            model = load_model( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ) )\n",
        "            accuracies.append( model.evaluate(X_fold_test, y_fold_test, batch_size=512, verbose=0 )[1] )\n",
        "        else:\n",
        "            model = pickle.load(open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ), 'rb'))\n",
        "            accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
        "        print( '{}: {}'.format(foldInd, accuracies[-1]) )\n",
        "        #\n",
        "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
        "    return model\n",
        "##################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uCLAFbkkDSJ",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBrid3zgkBG_",
        "colab_type": "code",
        "outputId": "8b4adb08-f7de-4275-f3b5-2f8e31a49cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "parameters = {\n",
        "    \"penalty\":[\"l2\"],\n",
        "    \"C\": [ 3., 4., 5.],\n",
        "    \"fit_intercept\": [True],\n",
        "    \"class_weight\":['balanced'],\n",
        "    \"solver\":[ 'lbfgs' ],\n",
        "    \"multi_class\": [\"multinomial\"],\n",
        "    \"random_state\":[77]\n",
        "    }\n",
        "LR = GridSearchCV(LogisticRegression(), \n",
        "                  parameters, \n",
        "                  cv=4, \n",
        "                  n_jobs=-1)\n",
        "\n",
        "LR = fitValidateSave( LR, 'LR' ) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.75\n",
            "1: 0.78125\n",
            "2: 0.8125\n",
            "3: 0.8125\n",
            "4: 0.8709677419354839\n",
            "5: 0.7096774193548387\n",
            "6: 0.8709677419354839\n",
            "7: 0.7741935483870968\n",
            "8: 0.9032258064516129\n",
            "9: 0.7741935483870968\n",
            "10: 0.8709677419354839\n",
            "11: 0.7741935483870968\n",
            "12: 0.7741935483870968\n",
            "13: 0.9354838709677419\n",
            "14: 0.7419354838709677\n",
            "15: 0.7419354838709677\n",
            "16: 0.7741935483870968\n",
            "17: 0.8387096774193549\n",
            "18: 0.6774193548387096\n",
            "19: 0.7419354838709677\n",
            "20: 0.9032258064516129\n",
            "21: 0.8064516129032258\n",
            "22: 0.6774193548387096\n",
            "23: 0.7419354838709677\n",
            "24: 0.8064516129032258\n",
            "25: 0.7741935483870968\n",
            "26: 0.8709677419354839\n",
            "27: 0.7419354838709677\n",
            "28: 0.7419354838709677\n",
            "29: 0.8387096774193549\n",
            "30: 0.8709677419354839\n",
            "31: 0.7096774193548387\n",
            "32: 0.8709677419354839\n",
            "33: 0.8709677419354839\n",
            "34: 0.7096774193548387\n",
            "35: 0.8064516129032258\n",
            "36: 0.8387096774193549\n",
            "37: 0.7419354838709677\n",
            "38: 0.7419354838709677\n",
            "39: 0.7419354838709677\n",
            "Average accuracy for LR is:  0.793422379032258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzpl-Y1ykGwl",
        "colab_type": "text"
      },
      "source": [
        "Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD4M74jDkE55",
        "colab_type": "code",
        "outputId": "469b44df-1acc-492d-a064-a2a2803d12f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "parameters = {\n",
        "    \"criterion\":[\"gini\"],\n",
        "    \"max_depth\":[ 15, 30  ],\n",
        "    \"min_samples_split\": [ 5 ],\n",
        "    \"min_samples_leaf\": [1],\n",
        "    \"max_features\":[None ],\n",
        "    \"random_state\": [77],\n",
        "    \"n_estimators\":[ 200 ]\n",
        "    }\n",
        "RF_gini = GridSearchCV(RandomForestClassifier(), \n",
        "                  parameters, \n",
        "                  cv=4, \n",
        "                  n_jobs=-1)\n",
        "\n",
        "RF_gini = fitValidateSave( RF_gini, 'RF_gini' ) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.8125\n",
            "1: 0.78125\n",
            "2: 0.84375\n",
            "3: 0.84375\n",
            "4: 0.9032258064516129\n",
            "5: 0.7419354838709677\n",
            "6: 0.8387096774193549\n",
            "7: 0.7419354838709677\n",
            "8: 0.9354838709677419\n",
            "9: 0.7096774193548387\n",
            "10: 0.8387096774193549\n",
            "11: 0.7741935483870968\n",
            "12: 0.8709677419354839\n",
            "13: 0.9354838709677419\n",
            "14: 0.7741935483870968\n",
            "15: 0.8387096774193549\n",
            "16: 0.7741935483870968\n",
            "17: 0.8387096774193549\n",
            "18: 0.7419354838709677\n",
            "19: 0.7419354838709677\n",
            "20: 0.8709677419354839\n",
            "21: 0.7741935483870968\n",
            "22: 0.7419354838709677\n",
            "23: 0.7096774193548387\n",
            "24: 0.7096774193548387\n",
            "25: 0.7096774193548387\n",
            "26: 0.8709677419354839\n",
            "27: 0.8387096774193549\n",
            "28: 0.8064516129032258\n",
            "29: 0.8709677419354839\n",
            "30: 0.8387096774193549\n",
            "31: 0.7096774193548387\n",
            "32: 0.8064516129032258\n",
            "33: 0.8064516129032258\n",
            "34: 0.7741935483870968\n",
            "35: 0.7419354838709677\n",
            "36: 0.9032258064516129\n",
            "37: 0.7741935483870968\n",
            "38: 0.6774193548387096\n",
            "39: 0.8387096774193549\n",
            "Average accuracy for RF_gini is:  0.8013860887096774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdm18KBkKLt",
        "colab_type": "text"
      },
      "source": [
        "Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVmQy8DRkIBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "6813da3c-20f5-48e8-c75c-a6a091a301f4"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "AB_gini = AdaBoostClassifier( base_estimator = DecisionTreeClassifier( \n",
        "                             criterion         = 'gini', \n",
        "                             splitter          = 'random',\n",
        "                             max_depth         = 30, \n",
        "                             min_samples_split = 5, \n",
        "                             min_samples_leaf  = 1,\n",
        "                             max_features      = None,\n",
        "                             random_state      = 77 \n",
        "                            ),\n",
        "                            learning_rate= 1,\n",
        "                            n_estimators = 200\n",
        "                         )\n",
        "AB_gini = fitValidateSave( AB_gini, 'AB_gini' )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.78125\n",
            "1: 0.8125\n",
            "2: 0.8125\n",
            "3: 0.875\n",
            "4: 0.8709677419354839\n",
            "5: 0.8064516129032258\n",
            "6: 0.8387096774193549\n",
            "7: 0.7419354838709677\n",
            "8: 0.9354838709677419\n",
            "9: 0.8064516129032258\n",
            "10: 0.8064516129032258\n",
            "11: 0.7096774193548387\n",
            "12: 0.8064516129032258\n",
            "13: 0.9354838709677419\n",
            "14: 0.8064516129032258\n",
            "15: 0.9354838709677419\n",
            "16: 0.8387096774193549\n",
            "17: 0.8709677419354839\n",
            "18: 0.7419354838709677\n",
            "19: 0.7096774193548387\n",
            "20: 0.8387096774193549\n",
            "21: 0.8387096774193549\n",
            "22: 0.6451612903225806\n",
            "23: 0.7741935483870968\n",
            "24: 0.8387096774193549\n",
            "25: 0.7741935483870968\n",
            "26: 0.7741935483870968\n",
            "27: 0.8387096774193549\n",
            "28: 0.7096774193548387\n",
            "29: 0.8387096774193549\n",
            "30: 0.8387096774193549\n",
            "31: 0.7096774193548387\n",
            "32: 0.8064516129032258\n",
            "33: 0.8709677419354839\n",
            "34: 0.7741935483870968\n",
            "35: 0.7741935483870968\n",
            "36: 0.9032258064516129\n",
            "37: 0.7741935483870968\n",
            "38: 0.7741935483870968\n",
            "39: 0.7096774193548387\n",
            "Average accuracy for AB_gini is:  0.8062247983870968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zHai65ikNzt",
        "colab_type": "text"
      },
      "source": [
        "XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ClYvUpXkMNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "99fe101c-1ef9-4c59-ba27-7753feefc8a7"
      },
      "source": [
        "XGB = XGBClassifier(  max_depth=6,  \n",
        "                      learning_rate=0.1, \n",
        "                      n_estimators=100, \n",
        "                      verbosity=1, \n",
        "                      objective='multi:softmax', \n",
        "                      num_class=y_cat.shape[-1],\n",
        "                      booster='gbtree', \n",
        "                      n_jobs=4, \n",
        "                      gamma=0, \n",
        "                      min_child_weight=1,\n",
        "                      max_delta_step=0, \n",
        "                      subsample=.7, \n",
        "                      colsample_bytree=.6, \n",
        "                      colsample_bylevel=.6, \n",
        "                      colsample_bynode=.6, \n",
        "                      reg_alpha=.0, \n",
        "                      reg_lambda=.0, \n",
        "                      scale_pos_weight=1, \n",
        "                      base_score=0.1, \n",
        "                      random_state=77 \n",
        "                      )\n",
        "XGB = fitValidateSave( XGB, 'XGB' )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.78125\n",
            "1: 0.75\n",
            "2: 0.78125\n",
            "3: 0.875\n",
            "4: 0.8709677419354839\n",
            "5: 0.7096774193548387\n",
            "6: 0.8709677419354839\n",
            "7: 0.7419354838709677\n",
            "8: 0.9354838709677419\n",
            "9: 0.7741935483870968\n",
            "10: 0.9032258064516129\n",
            "11: 0.7741935483870968\n",
            "12: 0.9032258064516129\n",
            "13: 0.967741935483871\n",
            "14: 0.7419354838709677\n",
            "15: 0.7419354838709677\n",
            "16: 0.7419354838709677\n",
            "17: 0.8387096774193549\n",
            "18: 0.7419354838709677\n",
            "19: 0.7741935483870968\n",
            "20: 0.8387096774193549\n",
            "21: 0.8064516129032258\n",
            "22: 0.6774193548387096\n",
            "23: 0.6451612903225806\n",
            "24: 0.7419354838709677\n",
            "25: 0.7419354838709677\n",
            "26: 0.8064516129032258\n",
            "27: 0.8064516129032258\n",
            "28: 0.7741935483870968\n",
            "29: 0.9354838709677419\n",
            "30: 0.7741935483870968\n",
            "31: 0.7096774193548387\n",
            "32: 0.7096774193548387\n",
            "33: 0.8709677419354839\n",
            "34: 0.7741935483870968\n",
            "35: 0.8064516129032258\n",
            "36: 0.9032258064516129\n",
            "37: 0.7741935483870968\n",
            "38: 0.7096774193548387\n",
            "39: 0.8709677419354839\n",
            "Average accuracy for XGB is:  0.797429435483871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_YAtdRBkQsm",
        "colab_type": "text"
      },
      "source": [
        "Neural Nets (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YPi_E_2kPOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c8add97-ae27-4f9c-a1da-b663ad4ada1f"
      },
      "source": [
        "##################################################\n",
        "def saveToH5( data , filePath , fillvalue=0 ):\n",
        "    h5f = h5py.File( filePath , 'w')\n",
        "    h5f.create_dataset('dataset', data =  data ,fillvalue=fillvalue ,compression='gzip', compression_opts=4 ); \n",
        "    h5f.close()\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "def loadFromH5( filePath ):\n",
        "    h5f = h5py.File( filePath , 'r')\n",
        "    output =   h5f['dataset'][:]  ; h5f.close()\n",
        "    return output\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "def buildMLP():\n",
        "    # DEFINE MLP MODEL\n",
        "    main_input = Input( shape=( X.shape[-1], ) ,  name = 'features' )\n",
        "    x = Dropout(0.8) (main_input)\n",
        "    #x = BatchNormalization(axis = -1)(main_input)\n",
        "    x = Dense( nodes, activation='relu',\n",
        "                           kernel_regularizer   =reg,\n",
        "                           activity_regularizer =reg,\n",
        "                           bias_regularizer     =reg\n",
        "              )(x)\n",
        "    x = Dropout( drops ) (x)\n",
        "    \n",
        "    ###\n",
        "    for lay in range(layers-1):\n",
        "        #\n",
        "        if True:\n",
        "            x = concatenate([x, main_input])\n",
        "        #\n",
        "        #x = BatchNormalization(axis = -1)(x)\n",
        "        x = Dense( nodes, activation='relu' )(x)\n",
        "        x = Dropout( drops ) (x)\n",
        "    ###       \n",
        "    output =  Dense( y_cat.shape[-1], activation='softmax', name = 'output' )(x)     \n",
        "    \n",
        "    ###\n",
        "    model = Model(input=main_input, output=output)\n",
        "    ###\n",
        "    model.compile( optimizer=optimizer , \n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['categorical_accuracy']\n",
        "                   )\n",
        "    ###\n",
        "    #model.summary()  \n",
        "    return model\n",
        "############\n",
        "\n",
        "############\n",
        "# TRAIN MODELS\n",
        "def trainMLP(modelName='NN'):\n",
        "    accuracies = []\n",
        "    for foldInd, fold in enumerate(folds[:]):\n",
        "        model = buildMLP()\n",
        "        X_fold = np.take( X, fold[0], axis=0)\n",
        "        y_fold = np.take( y_cat, fold[0], axis=0)\n",
        "        X_fold_test = np.take( X, fold[1], axis=0)\n",
        "        y_fold_test = np.take( y_cat, fold[1], axis=0)\n",
        "        \n",
        "        \n",
        "        loss_history =[] ;  no_improvement = 0 ; break_it = 0\n",
        "        for epok in range( 0 , 1000 ) :\n",
        "            if break_it :\n",
        "                break\n",
        "            \n",
        "            model.fit(X_fold, \n",
        "                      y_fold, \n",
        "                      batch_size=batchSize,\n",
        "                      shuffle=True,\n",
        "                      epochs=1, \n",
        "                      verbose=0,\n",
        "                      validation_data=(X_fold_test, y_fold_test)\n",
        "                      )\n",
        "            \n",
        "            \n",
        "            loss_history.append( model.evaluate( x=X_fold_test, y=y_fold_test, batch_size=512, verbose=2)[1] )\n",
        "            #\n",
        "            if len(loss_history)>1:\n",
        "                if loss_history[-1] <= max( loss_history[:-1] ):\n",
        "                    no_improvement +=1\n",
        "                else:\n",
        "                    no_improvement = 0\n",
        "                    model.save( '{}/{}_fold{}.h5'.format( args.modelsPath, modelName, foldInd )  )\n",
        "                #                            \n",
        "                if no_improvement >= 20:\n",
        "                    break_it = 1\n",
        "                    accuracies.append( max(loss_history) )\n",
        "                    print( '{}: {}'.format( foldInd, accuracies[-1] ) )\n",
        "                    break\n",
        "    print( 'Average accuracy for {} is:  {}'.format( 'NN', np.mean(accuracies)) )  \n",
        "################################################\n",
        "\n",
        "\n",
        "###########\n",
        "index_ = 0 \n",
        "#\n",
        "layers = 2\n",
        "nodes = 512\n",
        "drops = 0.5\n",
        "batchSize = 128\n",
        "lr=0.003\n",
        "optimizer = Adam(lr=lr)\n",
        "reg=l1_l2(l1=0.0001, l2=0.0005)\n",
        "#\n",
        "trainMLP( modelName='NN{}'.format(index_) )\n",
        "###########\n",
        "\n",
        "\n",
        "###########\n",
        "index_ = 1 \n",
        "#\n",
        "layers = 2\n",
        "nodes = 512\n",
        "drops = 0.5\n",
        "batchSize = 128\n",
        "lr=0.003\n",
        "optimizer = Adam(lr=lr)\n",
        "reg=l1_l2(l1=0.0001, l2=0.001)\n",
        "#\n",
        "trainMLP( modelName='NN{}'.format(index_) )\n",
        "###########"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"fe..., outputs=Tensor(\"ou...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: 0.78125\n",
            "1: 0.78125\n",
            "2: 0.8125\n",
            "3: 0.90625\n",
            "4: 0.8709677457809448\n",
            "5: 0.7419354915618896\n",
            "6: 0.9354838728904724\n",
            "7: 0.8064516186714172\n",
            "8: 0.9032257795333862\n",
            "9: 0.774193525314331\n",
            "10: 0.8709677457809448\n",
            "11: 0.8064516186714172\n",
            "12: 0.7096773982048035\n",
            "13: 0.9032257795333862\n",
            "14: 0.8064516186714172\n",
            "15: 0.7419354915618896\n",
            "16: 0.8064516186714172\n",
            "17: 0.9032257795333862\n",
            "18: 0.774193525314331\n",
            "19: 0.774193525314331\n",
            "20: 0.8064516186714172\n",
            "21: 0.8064516186714172\n",
            "22: 0.7096773982048035\n",
            "23: 0.8064516186714172\n",
            "24: 0.8387096524238586\n",
            "25: 0.8387096524238586\n",
            "26: 0.9354838728904724\n",
            "27: 0.8387096524238586\n",
            "28: 0.7096773982048035\n",
            "29: 0.8387096524238586\n",
            "30: 0.8387096524238586\n",
            "31: 0.8387096524238586\n",
            "32: 0.9032257795333862\n",
            "33: 0.8064516186714172\n",
            "34: 0.774193525314331\n",
            "35: 0.8709677457809448\n",
            "36: 0.8387096524238586\n",
            "37: 0.774193525314331\n",
            "38: 0.8387096524238586\n",
            "39: 0.8387096524238586\n",
            "Average accuracy for NN is:  0.8215473681688309\n",
            "0: 0.8125\n",
            "1: 0.75\n",
            "2: 0.8125\n",
            "3: 0.875\n",
            "4: 0.9032257795333862\n",
            "5: 0.774193525314331\n",
            "6: 0.9032257795333862\n",
            "7: 0.8064516186714172\n",
            "8: 0.9032257795333862\n",
            "9: 0.774193525314331\n",
            "10: 0.9032257795333862\n",
            "11: 0.774193525314331\n",
            "12: 0.7419354915618896\n",
            "13: 0.9032257795333862\n",
            "14: 0.8387096524238586\n",
            "15: 0.7419354915618896\n",
            "16: 0.8387096524238586\n",
            "17: 0.9032257795333862\n",
            "18: 0.7096773982048035\n",
            "19: 0.7419354915618896\n",
            "20: 0.8387096524238586\n",
            "21: 0.8064516186714172\n",
            "22: 0.6774193644523621\n",
            "23: 0.774193525314331\n",
            "24: 0.8387096524238586\n",
            "25: 0.8387096524238586\n",
            "26: 0.9354838728904724\n",
            "27: 0.8064516186714172\n",
            "28: 0.7096773982048035\n",
            "29: 0.774193525314331\n",
            "30: 0.8064516186714172\n",
            "31: 0.774193525314331\n",
            "32: 0.9032257795333862\n",
            "33: 0.8387096524238586\n",
            "34: 0.7419354915618896\n",
            "35: 0.8387096524238586\n",
            "36: 0.9032257795333862\n",
            "37: 0.774193525314331\n",
            "38: 0.8387096524238586\n",
            "39: 0.8709677457809448\n",
            "Average accuracy for NN is:  0.8175403088331222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1CzgdlkUeU",
        "colab_type": "text"
      },
      "source": [
        "Some more functions to generate the Ensemble prediction on Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS-lXKvHkdCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}