{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Training Trial\n",
    "This notebook is to support the [Tiny Training Trial](https://w.amazon.com/bin/view/MLSciences/Community/ML_Challenge/TinyTraining/). The associated Leaderboard is available [here](https://leaderboard.corp.amazon.com/tasks/292).\n",
    "\n",
    "## Loading the Data into Eider:\n",
    "Since we got some people asking about best ways to import into Eider, I thought we'd go one step further and make it trivial to import!  Below is a snippet for loading and and taking a look at the dataset via S3 below. It's highly recommended to use the below method to avoid a needless local import. \n",
    "\n",
    "First, let's make sure we have our credentials set to ```ml-eider-shared-1```, and then load them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download Training Data and Test Features ###\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "eider.s3.download('s3://eider-datasets/mlu/projects/DontOverFitChallenge/TTT_train.csv', '/tmp/TTT_train.csv')\n",
    "eider.s3.download('s3://eider-datasets/mlu/projects/DontOverFitChallenge/TTT_test_features.csv', '/tmp/TTT_test_features.csv')\n",
    "\n",
    "train = pd.read_csv('/tmp/TTT_train.csv')\n",
    "test = pd.read_csv('/tmp/TTT_test_features.csv')\n",
    "\n",
    "train_data = train\n",
    "unlabeled_data = test\n",
    "\n",
    "train_data = shuffle(train_data, random_state=11)\n",
    "\n",
    "### Let's See What We Are Up Against ###\n",
    "pd.options.display.max_columns = None\n",
    "# train.describe()\n",
    "# train_described = train.copy()\n",
    "# import numpy as np\n",
    "# train_described = train_described.replace(0, np.NaN)\n",
    "# train_described.describe().iloc[[0]]\n",
    "\n",
    "# Separate features and labels from the training set\n",
    "labels = train_data[['label']].copy()\n",
    "features = train_data.loc[:, train_data.columns != 'label'].copy()\n",
    "\n",
    "# Separate ids and features from the unlabeled set\n",
    "unlabeled_ids = unlabeled_data[['ID']].copy()\n",
    "unlabeled_features = unlabeled_data.loc[:, unlabeled_data.columns != 'ID'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the collisions from the training set\n",
    "\n",
    "Let's remove from the training set ($T_{old}$) \"very similar\" documents with the **different labels**.     \n",
    "The documents are \"very similar\" in case if the cosine similarity of the TF-IDF vectors is high **and** the euclidean distance is small.\n",
    "\n",
    "$T_{new} = T_{old} \\setminus T_{collisions}$    \n",
    "      \n",
    "where $T_{collisions} \\subset T_{old}$ and $\\forall x_i, x_j \\in T_{collisions}$:\n",
    "- $\\left\\Vert x_i - x_j \\right\\Vert \\leq 1$    \n",
    "- ${x_i \\cdot x_j^T \\over \\left\\Vert x_i \\right\\Vert \\cdot \\left\\Vert x_j \\right\\Vert} \\geq 0.85$     \n",
    "    \n",
    "- $label(x_i) \\neq label(x_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "cos_sim = cosine_similarity(features, features)\n",
    "euc_dists = euclidean_distances(features, features)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "lab = labels.values.ravel()\n",
    "cnt = 0\n",
    "\n",
    "removed = defaultdict(lambda: 0)\n",
    "removed_indices = []\n",
    "\n",
    "for i in range(1243):\n",
    "    for j in range(i + 1, 1244):\n",
    "        if cos_sim[i][j] > 0.85 and euc_dists[i][j] < 1 and lab[i] != lab[j]:\n",
    "            print('{} <==> {} ({} <=> {})'.format(i, j, lab[i], lab[j]))\n",
    "            cnt += 1\n",
    "            removed[lab[i]] += 1\n",
    "            removed[lab[j]] += 1\n",
    "            removed_indices.append(i)\n",
    "            removed_indices.append(j)\n",
    "\n",
    "print(cnt)\n",
    "print(removed)\n",
    "print(sum(removed.values()))\n",
    "print(sorted(removed_indices))\n",
    "\n",
    "features1 = features.copy()\n",
    "label1 = labels.copy()\n",
    "\n",
    "features1.drop(features1.index[removed_indices], inplace=True)\n",
    "label1.drop(label1.index[removed_indices], inplace=True)\n",
    "print(features1.shape)\n",
    "print(features.shape)\n",
    "\n",
    "print(label1.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "labels = label1\n",
    "features = features1\n",
    "\n",
    "train_set_size = labels.shape[0]\n",
    "print(train_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the matrix $X$, which contains the features of all documents (classified and unclassified)\n",
    "\n",
    "The matrix $X$ has 24'863 rows and 1'256 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all features together (from the training set and from the unlabeled set)\n",
    "X = features.copy().append(unlabeled_features)\n",
    "\n",
    "print(features.shape)\n",
    "print(unlabeled_features.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA visualization of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the features of the training set, using the LDA\n",
    "\n",
    "def pca_2d(X):\n",
    "    \n",
    "    import numpy as np\n",
    "    X = np.array(X)\n",
    "    \n",
    "    features = X[:train_set_size,:]\n",
    "    print(X.shape)\n",
    "    print(features.shape)\n",
    "    \n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    \n",
    "    random.seed(17)\n",
    "    colors = [(random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)) for _ in range(0, 10)]\n",
    "    \n",
    "    X2 = features.copy()\n",
    "    y = labels.values.ravel()\n",
    "    \n",
    "    # lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "    # X_r2 = lda.fit(X2, y).transform(X2)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    X_r2 = pca.transform(X2)\n",
    "    \n",
    "    plt.figure()\n",
    "    for color, i in zip(colors, range(0, 10)):\n",
    "        plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.2, color=color)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Visualize training set (with the extracted features) in 3D using the LDA\n",
    "\n",
    "def pca_3d(X, elev=60, azim=40):\n",
    "    \n",
    "    import numpy as np\n",
    "    X = np.array(X)\n",
    "    \n",
    "    features = X[:train_set_size,:]\n",
    "    print(X.shape)\n",
    "    print(features.shape)\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "    \n",
    "    X3 = features.copy()\n",
    "    y = labels.values.ravel()\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(7, 7))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=elev, azim=azim)\n",
    "    plt.cla()\n",
    "    \n",
    "    # lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "    # X3 = lda.fit(X3, y).transform(X3)\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(X)\n",
    "    X3 = pca.transform(X3)\n",
    "    \n",
    "    y = np.choose(y, range(0, 10)).astype(np.float)\n",
    "    ax.scatter(X3[:, 0], X3[:, 1], X3[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='') #, alpha=.2)\n",
    "    \n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "pca_2d(X)\n",
    "pca_3d(X, elev=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarization of the feature vectors\n",
    "\n",
    "Almost all documents have less than 10 words (many of them have 1-5 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize\n",
    "import numpy as np \n",
    "X1 = np.array(X.copy())\n",
    "X1[X1 > 0] = 1\n",
    "\n",
    "pca_2d(X1)\n",
    "pca_3d(X1, elev=40)\n",
    "\n",
    "X = X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the structure in the data\n",
    "\n",
    "      \n",
    "The matrix $X$ has 24'863 rows and 1'256 columns.     \n",
    "Let's apply the **lossy compression** to $X$ in order to reveal the presense of the internal structure in the data.     \n",
    "\n",
    "We apply the lossy compression to all feature vectors (labaled and unlabeled).       \n",
    "Afterwards we will check if the detected internal structure correlates to the labels from the training set.\n",
    "\n",
    "# Non-negative matrix factorization     \n",
    "\n",
    "Let's approximate $X$ as a product of two low-dimensional matrices: $X \\approx W \\times H$    \n",
    "\n",
    "Where:     \n",
    "- the matrix $W$ has 24'863 rows and 40 columns\n",
    "- the matrix $H$ has 40 rows and 1'256 columns\n",
    "\n",
    "Let's solve the following optimization problem:    \n",
    "- $min \\left\\Vert X - W H \\right\\Vert_F$     \n",
    "- subject to: $W \\geq 0$, $H \\geq 0$     \n",
    "   \n",
    "Where: $\\left\\Vert A \\right\\Vert_F = \\sqrt{ \\sum_{i=1}^N \\sum_{j=1}^N a_{ij}^2}$ is a Frobenius norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the matrix factorization with 10 hidden features\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=40, init='nndsvd', random_state=1, # verbose=2, \n",
    "            alpha=0.1, l1_ratio=0.5,\n",
    "            beta_loss='frobenius', solver='cd', max_iter=500, tol=0.000001)\n",
    "\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "\n",
    "W_orig = W.copy()\n",
    "\n",
    "# Print the shape of the documents matrix\n",
    "print(W.shape)\n",
    "\n",
    "# Extract from the documents matrix - the items, which correspond to the training set\n",
    "features_2 = W[:train_set_size,:]\n",
    "print(features_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF results visualization\n",
    "\n",
    "Given the approximation: $X \\approx W \\times H$\n",
    "\n",
    "For every 1'256-dimensional row-vector from the matrix $X$ - there is a corresponding 40-dimensional row-vector from the matrix $W$.\n",
    "\n",
    "Let's visualize the row-vectors of $W$, which correspond to the vectors of the training set.    \n",
    "Also, let's group the vectors, which have the same labels.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize (Documents x Hidden features) matrix (where documents are grouped by the class)\n",
    "# With 10 hidden features\n",
    "\n",
    "def visuzlize_latent_features(W):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(5, 12)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    max_for_all_W = np.amax(W)\n",
    "    \n",
    "    features_2_head = W[:train_set_size,:].copy()\n",
    "    \n",
    "    tuples = []\n",
    "    for row_label, row, lab in zip(range(features_2_head.shape[0]), features_2_head, labels.values.ravel()):\n",
    "        tuples.append((row, lab))\n",
    "    \n",
    "    # tuples.sort(key=lambda x: ' '.join(str(i) for i in x[1]))\n",
    "    tuples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    width = 20\n",
    "    prev_lab = None\n",
    "    row_num = 0\n",
    "    for row, lab in tuples:\n",
    "        if lab != prev_lab:\n",
    "            col_num = 0\n",
    "            for col in row:\n",
    "                rect1 = matplotlib.patches.Rectangle(\n",
    "                    (col_num * width, row_num * width), \n",
    "                    width, width * 2, \n",
    "                    color=(1, 0, 0, 1))\n",
    "                ax.add_patch(rect1)\n",
    "                col_num += 1\n",
    "            row_num += 2\n",
    "        # Normalization of the row\n",
    "        # norm = 0\n",
    "        # for col in row:\n",
    "        #     norm += col\n",
    "        norm = max_for_all_W\n",
    "        col_num = 0\n",
    "        for col in row:\n",
    "            rect1 = matplotlib.patches.Rectangle(\n",
    "                (col_num * width, row_num * width), \n",
    "                width, width, \n",
    "                color=(0, 0, 1, 1.0 / norm * col))\n",
    "            ax.add_patch(rect1)\n",
    "            col_num += 1\n",
    "        prev_lab = lab\n",
    "        row_num += 1\n",
    "        if row_num % 100 == 0:\n",
    "            print(row_num)\n",
    "    \n",
    "    plt.xlim([0, features_2_head.shape[1] * width])\n",
    "    plt.ylim([0, (features_2_head.shape[0] + 20) * width])\n",
    "    plt.show()\n",
    "    \n",
    "visuzlize_latent_features(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For every* class of documents we can observe the prevalent latent features\n",
    "\n",
    "\\* **Except the class 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional visualization + K-Means clustering of similar vectors of the latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize (Documents x Hidden features) matrix (where documents are grouped by the class) \n",
    "# and cluster the documents within the same class according to the hidden feature vectors.\n",
    "# With 10 hidden features\n",
    "\n",
    "def cluster_and_visuzlize_latent_features(W):\n",
    "    \n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "    from sklearn.preprocessing import normalize, scale\n",
    "    import math\n",
    "    \n",
    "    max_for_all_W = np.amax(W)\n",
    "    \n",
    "    features_2_head = W[:train_set_size,:].copy()\n",
    "    \n",
    "    tuples = []\n",
    "    \n",
    "    lab_to_row = dict()\n",
    "    for row, lab in zip(features_2_head, labels.values.ravel()):\n",
    "        if lab not in lab_to_row:\n",
    "            lab_to_row[lab] = list()\n",
    "        lab_to_row[lab].append(row)\n",
    "        \n",
    "    centroids = dict() # dict[label_class, list[tuple[vector, count]]]\n",
    "    lab_to_clust_cnt = {\n",
    "        0:3,\n",
    "        1:3,\n",
    "        2:3,\n",
    "        3:3,\n",
    "        4:3,\n",
    "        5:3,\n",
    "        6:3,\n",
    "        7:3,\n",
    "        8:3,\n",
    "        9:3\n",
    "    }\n",
    "    for lab in sorted(lab_to_row.keys()):\n",
    "        \n",
    "        centroids[lab] = list()\n",
    "        \n",
    "        arr = np.array(lab_to_row[lab])\n",
    "        \n",
    "        arr = normalize(arr.copy(), axis=1, norm='l2')\n",
    "        clust = KMeans(n_clusters=lab_to_clust_cnt[lab], random_state=0, max_iter=500).fit(arr)\n",
    "        # print(\"Label: {}\".format(lab))\n",
    "        center_to_assigned_points = list()\n",
    "        cluster_indices_count = dict()\n",
    "        for c, cluster_index in zip(clust.cluster_centers_, range(lab_to_clust_cnt[lab])):\n",
    "            # center_vector_length = math.sqrt(sum([x**2 for x in c]))\n",
    "            cluster_index_count = sum([1 if cluster_index == ll else 0 for ll in clust.labels_])\n",
    "            cluster_indices_count[cluster_index] = cluster_index_count\n",
    "            center_to_assigned_points.append((\n",
    "                [\"{:1.5f}\".format(x) for x in c], \n",
    "                cluster_index_count))\n",
    "            centroids[lab].append((c, cluster_index_count))\n",
    "        center_to_assigned_points.sort(key=lambda x: -x[1])\n",
    "        print(\"{}: [\".format(lab))\n",
    "        for center, points_cnt in center_to_assigned_points:\n",
    "            # print(\"[{}] \\t {}\".format(', '.join(center), points_cnt))\n",
    "            print(\"([{}], {}),\".format(', '.join(center), points_cnt))\n",
    "        print(\"],\")\n",
    "        print\n",
    "        \n",
    "        # clust = AgglomerativeClustering(n_clusters=5, affinity='cosine', linkage='average').fit(arr)\n",
    "        rows_and_cluster_indices = list(zip(lab_to_row[lab], clust.labels_))\n",
    "        # rows_and_cluster_indices.sort(key=lambda x: x[1])\n",
    "        rows_and_cluster_indices.sort(key=lambda x: -cluster_indices_count[x[1]])\n",
    "        sorted_rows = [(row, lab) for row, _ in rows_and_cluster_indices]\n",
    "        tuples.extend(sorted_rows)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(5, 12)\n",
    "    ax = fig.add_subplot(111)\n",
    "        \n",
    "    width = 20\n",
    "    prev_lab = None\n",
    "    row_num = 0\n",
    "    for row, lab in tuples:\n",
    "        if lab != prev_lab:\n",
    "            col_num = 0\n",
    "            for col in row:\n",
    "                rect1 = matplotlib.patches.Rectangle(\n",
    "                    (col_num * width, row_num * width), \n",
    "                    width, width * 2, \n",
    "                    color=(1, 0, 0, 1))\n",
    "                ax.add_patch(rect1)\n",
    "                col_num += 1\n",
    "            row_num += 2\n",
    "        # Normalization of the row\n",
    "        # norm = 0\n",
    "        # for col in row:\n",
    "        #     norm += col**2\n",
    "        norm = max_for_all_W\n",
    "        col_num = 0\n",
    "        for col in row:\n",
    "            rect1 = matplotlib.patches.Rectangle(\n",
    "                (col_num * width, row_num * width), \n",
    "                width, width, \n",
    "                color=(0, 0, 1, 1.0 / norm * col))\n",
    "            ax.add_patch(rect1)\n",
    "            col_num += 1\n",
    "        prev_lab = lab\n",
    "        row_num += 1\n",
    "        if row_num % 100 == 0:\n",
    "            print(row_num)\n",
    "    \n",
    "    plt.xlim([0, features_2_head.shape[1] * width])\n",
    "    plt.ylim([0, (features_2_head.shape[0] + 20) * width])\n",
    "    plt.show()\n",
    "    \n",
    "    # Collisions\n",
    "    print(\"\\n Collisions of centroids: \\n\")\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    for label1 in centroids.keys():\n",
    "        if label1 == 0:\n",
    "            continue\n",
    "        for label2 in centroids.keys():\n",
    "            if label2 <= label1:\n",
    "                continue\n",
    "            for centroid_1, cnt_1 in centroids[label1]:\n",
    "                for centroid_2, cnt_2 in centroids[label2]:    \n",
    "                    sim = cosine_similarity([centroid_1], [centroid_2])[0][0]\n",
    "                    if sim > 0.7:\n",
    "                        print(\"{} ({}) and {} ({}) ==> {} ==> {} and {}\".format(label1, cnt_1, label2, cnt_2, sim, centroid_1, centroid_2))\n",
    "                        \n",
    "cluster_and_visuzlize_latent_features(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the similarity of the latent features (\"topics\")\n",
    "\n",
    "\"Association\" (similarity) between the latent features: $S = W^T W$    \n",
    "The matrix $S$ has 40 rows and 40 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_matrices(W, H):\n",
    "\n",
    "    # features_2_head = W[:train_set_size,:].copy()\n",
    "    \n",
    "    # tuples = []\n",
    "    # for row_label, row, lab in zip(range(features_2_head.shape[0]), features_2_head, labels.values.ravel()):\n",
    "    #     tuples.append((row, lab))\n",
    "    \n",
    "    # # tuples.sort(key=lambda x: ' '.join(str(i) for i in x[1]))\n",
    "    # tuples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # # 2D matrix of documents\n",
    "    # rows_sorted_by_label = []\n",
    "    # for row, lab in tuples:\n",
    "    #     rows_sorted_by_label.append(row)\n",
    "        \n",
    "    # dim = len(rows_sorted_by_label)\n",
    "        \n",
    "    # rows_sorted_by_label = np.array(rows_sorted_by_label)  \n",
    "    \n",
    "    # from sklearn.preprocessing import normalize\n",
    "    # # rows_sorted_by_label = normalize(rows_sorted_by_label, norm='l2', axis=1)\n",
    "    \n",
    "    # displayed = rows_sorted_by_label.dot(rows_sorted_by_label.T)\n",
    "    \n",
    "    # plt.figure(figsize=(10,10))\n",
    "    # plt.imshow(displayed, interpolation='none', alpha=1)\n",
    "    # plt.colorbar()\n",
    "    # plt.title(\"Association of Documents\")\n",
    "    # plt.xlabel('Document index')\n",
    "    \n",
    "    # # Display delimiters\n",
    "    # prev_lab = None\n",
    "    # row_idx = 0\n",
    "    # for row, lab in tuples:\n",
    "    #     if prev_lab is not None and lab != prev_lab:\n",
    "    #         plt.plot([0,dim], [row_idx, row_idx], color='y',linewidth=0.5)\n",
    "    #         plt.plot([row_idx, row_idx], [0,dim], color='y',linewidth=0.5)\n",
    "    #     row_idx += 1\n",
    "    #     prev_lab = lab\n",
    "    \n",
    "    # plt.show()\n",
    "    \n",
    "    # http://jpfairbanks.net/2017/07/15/email-topics-with-nmf/\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(W.T.dot(W), interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Association of Topics (W'W)\")\n",
    "    plt.xlabel('Topic index')\n",
    "    \n",
    "    # plt.figure(figsize=(7,7))\n",
    "    # plt.imshow(H.dot(H.T), interpolation='none')\n",
    "    # plt.colorbar()\n",
    "    # plt.title(\"Association of Topics (HH')\")\n",
    "    # plt.xlabel('Topic index')\n",
    "\n",
    "W = W_orig.copy()\n",
    "display_matrices(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending the extracted latent features to the existing feature-vectors\n",
    "\n",
    "$X_{new} = [ \\  X \\ \\  \\   W \\  ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append columns with extracted features to the columns with original features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "W = W_orig.copy()\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "W = normalize(W, norm='l2', axis=1)\n",
    "\n",
    "# Print the shape of the documents matrix\n",
    "print(\"Extracted features (labeled + unlabeled) matrix dimension: {}\".format(W.shape))\n",
    "print(\"Original features (labeled + unlabeled) matrix dimension: {}\".format(X.shape))\n",
    "\n",
    "# W = np.concatenate((W_orig.copy(), X), axis=1)\n",
    "\n",
    "W = np.concatenate((W, X), axis=1)\n",
    "\n",
    "print(\"Concatenated (extracted + original) features matrix dimension: {}\".format(W.shape))\n",
    "# Extract from the documents matrix - the items, which correspond to the training set\n",
    "features_2 = W[:train_set_size,:]\n",
    "print(\"New labeled features dimension: {}\".format(features_2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA visualization of $X_{new}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d(W)\n",
    "pca_3d(W, elev=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the ensemble of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "cls = Pipeline([\n",
    "  ('cls', VotingClassifier(estimators=[\n",
    "        ('svm', svm.SVC(random_state=11, \n",
    "                        decision_function_shape='ovr',\n",
    "                        kernel='linear',\n",
    "                        class_weight='balanced',\n",
    "                        C=1,\n",
    "                        cache_size=300,\n",
    "                        probability=True)), \n",
    "        ('rf', RandomForestClassifier(random_state=11, \n",
    "                                      bootstrap=True,\n",
    "                                      max_depth=None,\n",
    "                                      max_features='log2',\n",
    "                                      min_samples_leaf=1,\n",
    "                                      n_estimators=1500,\n",
    "                                      class_weight='balanced',\n",
    "                                      criterion='gini',\n",
    "                                      n_jobs=8)),\n",
    "    ], \n",
    "    voting='soft',\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split labeled features into the training and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_2, labels, test_size=0.4, random_state=109)\n",
    "X_train.shape\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "# Cross-validation:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(cls, features_2, labels.values.ravel(), cv=5, n_jobs=5, verbose=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "cls.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = cls.predict(X_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier on 60% of the training set and evaluation on the rest 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(8, 8)\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot confusion matrices\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=False,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier on the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(features_2, labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_features = W[train_set_size:,:]\n",
    "unlabeled_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = cls\n",
    "\n",
    "#Predict the response for test dataset\n",
    "unlabeled_pred = classifier.predict(unlabeled_features)\n",
    "\n",
    "results_path = '/tmp/submission_NB44R392BHPZ_demo.csv'\n",
    "with open(results_path, 'w') as f:\n",
    "    f.write('ID,label\\n')\n",
    "    processed = 0\n",
    "    for row_id, row_pred in zip(unlabeled_ids['ID'], unlabeled_pred):\n",
    "        f.write(\"{},{}\\n\".format(row_id, row_pred))\n",
    "        processed += 1\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-10 most relevant tokens for every topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(topic, n_top_words):\n",
    "    return topic.argsort()[:-n_top_words - 1:-1]\n",
    "def topic_table(model, feature_names, n_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        t = (\"topic_%d:\" % topic_idx)\n",
    "        topics[t] = [feature_names[i] for i in top_words(topic, n_top_words)]\n",
    "    return pd.DataFrame(topics)\n",
    "\n",
    "topic_table(model, features.columns.tolist(), 20).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our model output out of Eider and into Leaderboard\n",
    "Great. Now we have a dummie sample submission in Eider that we now need to export locally so that we may then upload to Leaderboard in the following steps:\n",
    "1. Within the Eider console top bar, select [Files](https://eider.corp.amazon.com/file)\n",
    "2. You should now see 'Files', 'TMP' and 'Exported notebooks' tabs. \n",
    "3. Select 'TMP' then select 'Connect to workspace'. You should now see any files from your last run of your workspace. If there was no 'Connect to workspace' option, your files from the last run should already be present. *Files in the 'TMP' should be considered temporary as they will expire after an hour's worth of idle time.*\n",
    "4. Go to the ```TTT_fake_sub.csv``` file and select Save\n",
    "5. This file will now be permanently saved to your Eider account and available for local download.\n",
    "6. Go to the 'Files' tab, and click 'download' to save it to your local machine.\n",
    "\n",
    "We now have our model's output .csv and are ready to upload to Leaderboard\n",
    "1. Search for your [Leaderboard instance](https://leaderboard.corp.amazon.com/tasks/292) and go to the 'Make a Submission' section\n",
    "2. Upload your local file and include your notebook version URL for tracking.\n",
    "3. Your score on the public leaderboard should now appear. \n",
    "\n",
    "The private leaderboard contains the vast majority of the data, and so your final rankings in this competition will be a bit of a surprise! Take care and avoid overfitting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
