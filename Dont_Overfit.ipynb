{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dont_Overfit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vOzYDAlt43c15yP9xaKb0p2GgROtpYo3",
      "authorship_tag": "ABX9TyPRrd5OlY5kYcv07//sHJ0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solharsh/Experimenting_ML/blob/master/Dont_Overfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkyCXtE2W8nm",
        "colab_type": "text"
      },
      "source": [
        "# Avoid overfitting with a tiny sliver for training data\n",
        "Inspired by the Kaggle Don’t Overfit Challenge: Tiny Training Trial. The challenge; build the best performing model you can with a <5% training vs >95% test split with TF-IDF encodings on an Amazon multi-classification problem. With so many data hungry algorithms out there that take days or more to compute, we thought it’d be refreshing to go the other way and experiment with what can be done with extremely small and noisy datasets! Iterate and experiment with training times on the order of seconds. Our split is:\n",
        "\n",
        "Train: 1244 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kRUo1GtYAqr",
        "colab_type": "text"
      },
      "source": [
        "Approach overview\n",
        "\n",
        "•Build Ensemble that includes multiple model categories: Logistic Regression, Random Forests, XGBoost, Adaboost, and Neural Networks.\n",
        "\n",
        "•Split the training dataset into K stratified folds. For each fold and model category, train a separate model using Grid Search.\n",
        "\n",
        "•Combine all models into ensemble using Averaging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8AtqlR0YF-4",
        "colab_type": "text"
      },
      "source": [
        "I experimented with:\n",
        "\n",
        "- 1.Which model categories to include in the ensemble \n",
        "- 2.How many stratified folds to use: 1, 5, 10, 20, 40 \n",
        "- 3.How to build the ensemble: Averaging vs. Max voting\n",
        "- 4.Oversampling techniques such as SMOTE and ADASYN: including models trained with SMOTE data in the ensemble worked for the Public leaderboad, but not for Private\n",
        "- 5.Feature standardization: did not seem to improve anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rROiMlbEYNvo",
        "colab_type": "text"
      },
      "source": [
        "## Lessons Learned\n",
        "\n",
        "-  Ensembling is the way to go, of course.\n",
        "-  Increasing the number of stratified folds improved performance.\n",
        "-  Improvements in training data accuracy (on validation set) did not necessarrily translate to better accuracies in the Public dataset. A prime example for this was the LR method that did not perform as well in the training validation accuracy compared to other methods such as NN. However, LR was an integral part of the overall Ensemble; whenever we removed it, the Public dataset accuracy ended up much worse.\n",
        "-  Ensembling using Averaging always worked better than Max voting.\n",
        "-  We kind of `overfitted' to the Public Leaderboard, i.e., our best performing model in Public was not the best in Private. \n",
        "-  Adding models trained with oversampled data, using either SMOTE or ADASYN, decreased accuracy in Private dataset. \n",
        "-  Gini impurity appeared to work better than Entropy for tree-based models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kobTioBYWrf",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hprKn72iW2qE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "66043581-fac6-4d2b-f04a-2fafb2f78a11"
      },
      "source": [
        "import pandas as pd, numpy as np, time, sys, h5py\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from keras.layers import Input, Dense , Dropout , TimeDistributed , LSTM , GRU, concatenate, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD , Adadelta, RMSprop, Adam, Adamax\n",
        "from keras.models import  load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import  to_categorical \n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import pickle\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Wfv0YzYZxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize problem parameters\n",
        "class Args:\n",
        "    \"\"\" Class containing all model arguments \"\"\"\n",
        "    def __init__( self ):\n",
        "        self.project    = 'MLchallenge_DontOverfit'\n",
        "        self.dataPath   = '/content/drive/My Drive/Dont_Overfit/'       .format(self.project)\n",
        "        self.modelsPath = '/content/drive/My Drive/Dont_Overfit/' .format(self.project)\n",
        "        self.resultsPath= '/content/drive/My Drive/Dont_Overfit/'.format(self.project)\n",
        "        self.CV_folds   = 40  # split the Training data in stratified folds, to train different versions of models \n",
        "args = Args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaX3yoMMYgaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8f546ce2-2162-4c6a-c850-8f76e78a1251"
      },
      "source": [
        "# LOAD DATA\n",
        "train = pd.read_csv( args.dataPath + 'TTT_train.csv' )\n",
        "test  = pd.read_csv( args.dataPath + 'TTT_test_features.csv',index_col = 'ID')\n",
        "train.describe()\n",
        "X = train.loc[:, train.columns != 'label']\n",
        "y = train['label']\n",
        "y_cat = to_categorical(y)\n",
        "# Generate a set of stratified folds of Training to train different versions of each model.\n",
        "folds = list(StratifiedKFold(n_splits=args.CV_folds, shuffle=True, random_state=1).split(X, y))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 36 members, which is less than n_splits=40.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMQok9B1adhh",
        "colab_type": "text"
      },
      "source": [
        "## Some functions for model training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMLCyP3cYx65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################\n",
        "# function to fit a model on every fold, and store trained model\n",
        "def fitValidateSave( model, modelType ):\n",
        "    #\n",
        "    accuracies = []\n",
        "    for foldIndex, fold in enumerate(folds):\n",
        "        X_fold      = np.take( X, fold[0], axis=0)\n",
        "        y_fold      = np.take( y, fold[0], axis=0)\n",
        "        #\n",
        "        #oversampler = RandomOverSampler(random_state=77)\n",
        "        #X_fold, y_fold = oversampler.fit_sample(X_fold, y_fold)\n",
        "        #\n",
        "        X_fold_test = np.take( X, fold[1], axis=0)\n",
        "        y_fold_test = np.take( y, fold[1], axis=0)\n",
        "        #\n",
        "        model.fit(X_fold, y_fold)\n",
        "        #\n",
        "        accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
        "        print( '{}: {}'.format(foldIndex, accuracies[-1]) )\n",
        "        print(model.best_params_)\n",
        "        #\n",
        "        pickle.dump( model, open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldIndex ) , 'wb'))\n",
        "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
        "    return model\n",
        "##################################################\n",
        "\n",
        "\n",
        "##################################################\n",
        "# Compute accuracies across folds using an already trained model.\n",
        "def validateAcrossFolds( modelType ):\n",
        "    #\n",
        "    accuracies = []\n",
        "    for foldInd, fold in enumerate(folds):\n",
        "        X_fold_test = np.take( X, fold[1], axis=0)\n",
        "        y_fold_test = np.take( y, fold[1], axis=0)\n",
        "        #\n",
        "        if 'NN' in modelType:\n",
        "            y_fold_test = to_categorical(y_fold_test)\n",
        "            model = load_model( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ) )\n",
        "            accuracies.append( model.evaluate(X_fold_test, y_fold_test, batch_size=512, verbose=0 )[1] )\n",
        "        else:\n",
        "            model = pickle.load(open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ), 'rb'))\n",
        "            accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
        "        print( '{}: {}'.format(foldInd, accuracies[-1]) )\n",
        "        #\n",
        "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
        "    return model\n",
        "##################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vodUupMJai6q",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JmJtHZRafaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24ebd5c0-1e8b-4093-8eac-d7785e3b04a0"
      },
      "source": [
        "parameters = {\n",
        "    \"penalty\":[\"l2\"],\n",
        "    \"C\": [ 3., 4., 5.],\n",
        "    \"fit_intercept\": [True],\n",
        "    \"class_weight\":['balanced'],\n",
        "    \"solver\":[ 'lbfgs' ],\n",
        "    \"multi_class\": [\"multinomial\"],\n",
        "    \"random_state\":[77]\n",
        "    }\n",
        "LR = GridSearchCV(LogisticRegression(), \n",
        "                  parameters, \n",
        "                  cv=4, \n",
        "                  n_jobs=-1)\n",
        "\n",
        "LR = fitValidateSave( LR, 'LR' ) "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.75\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "1: 0.78125\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "2: 0.8125\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "3: 0.8125\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "4: 0.8709677419354839\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "5: 0.7096774193548387\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "6: 0.8709677419354839\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "7: 0.7741935483870968\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "8: 0.9032258064516129\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "9: 0.7741935483870968\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "10: 0.8709677419354839\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "11: 0.7741935483870968\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "12: 0.7741935483870968\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "13: 0.9354838709677419\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "14: 0.7419354838709677\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "15: 0.7419354838709677\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "16: 0.7741935483870968\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "17: 0.8387096774193549\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "18: 0.6774193548387096\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "19: 0.7419354838709677\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "20: 0.9032258064516129\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "21: 0.8064516129032258\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "22: 0.6774193548387096\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "23: 0.7419354838709677\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "24: 0.8064516129032258\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "25: 0.7741935483870968\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "26: 0.8709677419354839\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "27: 0.7419354838709677\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "28: 0.7419354838709677\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "29: 0.8387096774193549\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "30: 0.8709677419354839\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "31: 0.7096774193548387\n",
            "{'C': 3.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "32: 0.8709677419354839\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "33: 0.8709677419354839\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "34: 0.7096774193548387\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "35: 0.8064516129032258\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "36: 0.8387096774193549\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "37: 0.7419354838709677\n",
            "{'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "38: 0.7419354838709677\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "39: 0.7419354838709677\n",
            "{'C': 4.0, 'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'penalty': 'l2', 'random_state': 77, 'solver': 'lbfgs'}\n",
            "Average accuracy for LR is:  0.793422379032258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6H65zf4alrI",
        "colab_type": "text"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIJt15QJakJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00a56131-3149-41f9-8afb-a3dfbce1d6f6"
      },
      "source": [
        "parameters = {\n",
        "    \"criterion\":[\"gini\"],\n",
        "    \"max_depth\":[ 15, 30  ],\n",
        "    \"min_samples_split\": [ 5 ],\n",
        "    \"min_samples_leaf\": [1],\n",
        "    \"max_features\":[None ],\n",
        "    \"random_state\": [77],\n",
        "    \"n_estimators\":[ 200 ]\n",
        "    }\n",
        "RF_gini = GridSearchCV(RandomForestClassifier(), \n",
        "                  parameters, \n",
        "                  cv=4, \n",
        "                  n_jobs=-1)\n",
        "\n",
        "RF_gini = fitValidateSave( RF_gini, 'RF_gini' ) "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.8125\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "1: 0.78125\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "2: 0.84375\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "3: 0.84375\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "4: 0.9032258064516129\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "5: 0.7419354838709677\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "6: 0.8387096774193549\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "7: 0.7419354838709677\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "8: 0.9354838709677419\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "9: 0.7096774193548387\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "10: 0.8387096774193549\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "11: 0.7741935483870968\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "12: 0.8709677419354839\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "13: 0.9354838709677419\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "14: 0.7741935483870968\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "15: 0.8387096774193549\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "16: 0.7741935483870968\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "17: 0.8387096774193549\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "18: 0.7419354838709677\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "19: 0.7419354838709677\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "20: 0.8709677419354839\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "21: 0.7741935483870968\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "22: 0.7419354838709677\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "23: 0.7096774193548387\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "24: 0.7096774193548387\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "25: 0.7096774193548387\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "26: 0.8709677419354839\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "27: 0.8387096774193549\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "28: 0.8064516129032258\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "29: 0.8709677419354839\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "30: 0.8387096774193549\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "31: 0.7096774193548387\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "32: 0.8064516129032258\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "33: 0.8064516129032258\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "34: 0.7741935483870968\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "35: 0.7419354838709677\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "36: 0.9032258064516129\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "37: 0.7741935483870968\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "38: 0.6774193548387096\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "39: 0.8387096774193549\n",
            "{'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 77}\n",
            "Average accuracy for RF_gini is:  0.8013860887096774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVZOyqzIa0XY",
        "colab_type": "text"
      },
      "source": [
        "## Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v1KYaIfam1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "293a6137-253c-4196-b119-d5cbbacbd327"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "AB_gini = AdaBoostClassifier( base_estimator = DecisionTreeClassifier( \n",
        "                             criterion         = 'gini', \n",
        "                             splitter          = 'random',\n",
        "                             max_depth         = 30, \n",
        "                             min_samples_split = 5, \n",
        "                             min_samples_leaf  = 1,\n",
        "                             max_features      = None,\n",
        "                             random_state      = 77 \n",
        "                            ),\n",
        "                            learning_rate= 1,\n",
        "                            n_estimators = 200\n",
        "                         )\n",
        "AB_gini = fitValidateSave( AB_gini, 'AB_gini' )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.78125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-746814286057>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                             \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                          )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mAB_gini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitValidateSave\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mAB_gini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AB_gini'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-0ccba4122799>\u001b[0m in \u001b[0;36mfitValidateSave\u001b[0;34m(model, modelType)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fold_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fold_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'{}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'{}/{}_fold{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelsPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldIndex\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'AdaBoostClassifier' object has no attribute 'best_params_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YKxISrXa3Eu",
        "colab_type": "text"
      },
      "source": [
        "## XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv2Srscba1aB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "ac848a1f-29c9-4a65-cac9-11e4e027552e"
      },
      "source": [
        "XGB = XGBClassifier(  max_depth=6,  \n",
        "                      learning_rate=0.1, \n",
        "                      n_estimators=100, \n",
        "                      verbosity=1, \n",
        "                      objective='multi:softmax', \n",
        "                      num_class=y_cat.shape[-1],\n",
        "                      booster='gbtree', \n",
        "                      n_jobs=4, \n",
        "                      gamma=0, \n",
        "                      min_child_weight=1,\n",
        "                      max_delta_step=0, \n",
        "                      subsample=.7, \n",
        "                      colsample_bytree=.6, \n",
        "                      colsample_bylevel=.6, \n",
        "                      colsample_bynode=.6, \n",
        "                      reg_alpha=.0, \n",
        "                      reg_lambda=.0, \n",
        "                      scale_pos_weight=1, \n",
        "                      base_score=0.1, \n",
        "                      random_state=77 \n",
        "                      )\n",
        "XGB = fitValidateSave( XGB, 'XGB' )"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.78125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-3e3a318f316f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                       \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                       )\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mXGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitValidateSave\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mXGB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGB'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-0ccba4122799>\u001b[0m in \u001b[0;36mfitValidateSave\u001b[0;34m(model, modelType)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fold_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fold_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'{}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'{}/{}_fold{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelsPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldIndex\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'best_params_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyJjeCwna-PQ",
        "colab_type": "text"
      },
      "source": [
        "## Neural Nets (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CAXv25Ka4Gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5598a0f3-3dab-4147-8470-36e0f92847f5"
      },
      "source": [
        "##################################################\n",
        "def saveToH5( data , filePath , fillvalue=0 ):\n",
        "    h5f = h5py.File( filePath , 'w')\n",
        "    h5f.create_dataset('dataset', data =  data ,fillvalue=fillvalue ,compression='gzip', compression_opts=4 ); \n",
        "    h5f.close()\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "def loadFromH5( filePath ):\n",
        "    h5f = h5py.File( filePath , 'r')\n",
        "    output =   h5f['dataset'][:]  ; h5f.close()\n",
        "    return output\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "def buildMLP():\n",
        "    # DEFINE MLP MODEL\n",
        "    main_input = Input( shape=( X.shape[-1], ) ,  name = 'features' )\n",
        "    x = Dropout(0.8) (main_input)\n",
        "    #x = BatchNormalization(axis = -1)(main_input)\n",
        "    x = Dense( nodes, activation='relu',\n",
        "                           kernel_regularizer   =reg,\n",
        "                           activity_regularizer =reg,\n",
        "                           bias_regularizer     =reg\n",
        "              )(x)\n",
        "    x = Dropout( drops ) (x)\n",
        "    \n",
        "    ###\n",
        "    for lay in range(layers-1):\n",
        "        #\n",
        "        if True:\n",
        "            x = concatenate([x, main_input])\n",
        "        #\n",
        "        #x = BatchNormalization(axis = -1)(x)\n",
        "        x = Dense( nodes, activation='relu' )(x)\n",
        "        x = Dropout( drops ) (x)\n",
        "    ###       \n",
        "    output =  Dense( y_cat.shape[-1], activation='softmax', name = 'output' )(x)     \n",
        "    \n",
        "    ###\n",
        "    model = Model(input=main_input, output=output)\n",
        "    ###\n",
        "    model.compile( optimizer=optimizer , \n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['categorical_accuracy']\n",
        "                   )\n",
        "    ###\n",
        "    #model.summary()  \n",
        "    return model\n",
        "############\n",
        "\n",
        "############\n",
        "# TRAIN MODELS\n",
        "def trainMLP(modelName='NN'):\n",
        "    accuracies = []\n",
        "    for foldInd, fold in enumerate(folds[:]):\n",
        "        model = buildMLP()\n",
        "        X_fold = np.take( X, fold[0], axis=0)\n",
        "        y_fold = np.take( y_cat, fold[0], axis=0)\n",
        "        X_fold_test = np.take( X, fold[1], axis=0)\n",
        "        y_fold_test = np.take( y_cat, fold[1], axis=0)\n",
        "        \n",
        "        \n",
        "        loss_history =[] ;  no_improvement = 0 ; break_it = 0\n",
        "        for epok in range( 0 , 1000 ) :\n",
        "            if break_it :\n",
        "                break\n",
        "            \n",
        "            model.fit(X_fold, \n",
        "                      y_fold, \n",
        "                      batch_size=batchSize,\n",
        "                      shuffle=True,\n",
        "                      epochs=1, \n",
        "                      verbose=0,\n",
        "                      validation_data=(X_fold_test, y_fold_test)\n",
        "                      )\n",
        "            \n",
        "            \n",
        "            loss_history.append( model.evaluate( x=X_fold_test, y=y_fold_test, batch_size=512, verbose=2)[1] )\n",
        "            #\n",
        "            if len(loss_history)>1:\n",
        "                if loss_history[-1] <= max( loss_history[:-1] ):\n",
        "                    no_improvement +=1\n",
        "                else:\n",
        "                    no_improvement = 0\n",
        "                    model.save( '{}/{}_fold{}.h5'.format( args.modelsPath, modelName, foldInd )  )\n",
        "                #                            \n",
        "                if no_improvement >= 20:\n",
        "                    break_it = 1\n",
        "                    accuracies.append( max(loss_history) )\n",
        "                    print( '{}: {}'.format( foldInd, accuracies[-1] ) )\n",
        "                    break\n",
        "    print( 'Average accuracy for {} is:  {}'.format( 'NN', np.mean(accuracies)) )  \n",
        "################################################\n",
        "\n",
        "\n",
        "###########\n",
        "index_ = 0 \n",
        "#\n",
        "layers = 2\n",
        "nodes = 512\n",
        "drops = 0.5\n",
        "batchSize = 128\n",
        "lr=0.003\n",
        "optimizer = Adam(lr=lr)\n",
        "reg=l1_l2(l1=0.0001, l2=0.0005)\n",
        "#\n",
        "trainMLP( modelName='NN{}'.format(index_) )\n",
        "###########\n",
        "\n",
        "\n",
        "###########\n",
        "index_ = 1 \n",
        "#\n",
        "layers = 2\n",
        "nodes = 512\n",
        "drops = 0.5\n",
        "batchSize = 128\n",
        "lr=0.003\n",
        "optimizer = Adam(lr=lr)\n",
        "reg=l1_l2(l1=0.0001, l2=0.001)\n",
        "#\n",
        "trainMLP( modelName='NN{}'.format(index_) )\n",
        "###########"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"fe..., outputs=Tensor(\"ou...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: 0.8125\n",
            "1: 0.75\n",
            "2: 0.84375\n",
            "3: 0.90625\n",
            "4: 0.8387096524238586\n",
            "5: 0.7419354915618896\n",
            "6: 0.8709677457809448\n",
            "7: 0.8064516186714172\n",
            "8: 0.9032257795333862\n",
            "9: 0.774193525314331\n",
            "10: 0.8387096524238586\n",
            "11: 0.8064516186714172\n",
            "12: 0.6774193644523621\n",
            "13: 0.9354838728904724\n",
            "14: 0.8064516186714172\n",
            "15: 0.774193525314331\n",
            "16: 0.8387096524238586\n",
            "17: 0.8709677457809448\n",
            "18: 0.7096773982048035\n",
            "19: 0.774193525314331\n",
            "20: 0.9032257795333862\n",
            "21: 0.774193525314331\n",
            "22: 0.6774193644523621\n",
            "23: 0.774193525314331\n",
            "24: 0.8064516186714172\n",
            "25: 0.8387096524238586\n",
            "26: 0.9354838728904724\n",
            "27: 0.8064516186714172\n",
            "28: 0.7096773982048035\n",
            "29: 0.8709677457809448\n",
            "30: 0.8387096524238586\n",
            "31: 0.8064516186714172\n",
            "32: 0.9032257795333862\n",
            "33: 0.8709677457809448\n",
            "34: 0.7419354915618896\n",
            "35: 0.8387096524238586\n",
            "36: 0.8709677457809448\n",
            "37: 0.774193525314331\n",
            "38: 0.8387096524238586\n",
            "39: 0.8387096524238586\n",
            "Average accuracy for NN is:  0.8174899101257325\n",
            "0: 0.8125\n",
            "1: 0.75\n",
            "2: 0.8125\n",
            "3: 0.90625\n",
            "4: 0.8709677457809448\n",
            "5: 0.774193525314331\n",
            "6: 0.8709677457809448\n",
            "7: 0.774193525314331\n",
            "8: 0.9032257795333862\n",
            "9: 0.774193525314331\n",
            "10: 0.9032257795333862\n",
            "11: 0.774193525314331\n",
            "12: 0.7096773982048035\n",
            "13: 0.8709677457809448\n",
            "14: 0.774193525314331\n",
            "15: 0.774193525314331\n",
            "16: 0.8387096524238586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWEK-a-AbDY2",
        "colab_type": "text"
      },
      "source": [
        "## Some more functions to generate the Ensemble prediction on Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6-mbD99bBIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################\n",
        "def genTestPredictionsPerModelInstance( modelInstance ):\n",
        "    # Generate predictions over test set\n",
        "    predictions = np.zeros(( testData.shape[0], y_cat.shape[-1] ))\n",
        "    for foldIndex, fold in enumerate(folds[:]):\n",
        "        print(foldIndex)\n",
        "        if 'NN' in modelInstance:\n",
        "            model = load_model( '{}/{}_fold{}.h5'.format( args.modelsPath, modelInstance, foldIndex ) )\n",
        "            predictions += model.predict(testData, batch_size=1024) \n",
        "        else:\n",
        "            model = pickle.load(open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelInstance, foldIndex ), 'rb'))\n",
        "            predictions += model.predict_proba(testData)\n",
        "    #\n",
        "    predictionsPath = '{}/predictions_{}.h5'.format( args.resultsPath, modelInstance )\n",
        "    saveToH5( predictions, predictionsPath ) \n",
        "    return model\n",
        "##################################################\n",
        "\n",
        "\n",
        "##################################################\n",
        "# Get predictions \n",
        "def generateEnsemblePredictions(  ensemble = [ 'LR', 'RF_gini', 'AB_gini', 'NN', 'GB' ],                   \n",
        "                                  mode='sum'\n",
        "                                ): \n",
        "    #                      \n",
        "    ensemblePredictions = np.zeros(( testData.shape[0], len(ensemble), y_cat.shape[-1] ))\n",
        "    for modelInstanceIndex, modelInstance in enumerate(ensemble) :\n",
        "        print(modelInstance)\n",
        "        predictionsPath  = '{}/predictions_{}.h5'.format( args.resultsPath, modelInstance ) \n",
        "        modelPredictions = loadFromH5(predictionsPath)\n",
        "        ensemblePredictions[:,modelInstanceIndex] += np.copy( modelPredictions ) \n",
        "    #\n",
        "    if mode=='sum':\n",
        "        classPredictions = np.sum(ensemblePredictions, axis=1)   \n",
        "    elif mode=='max':\n",
        "        classPredictions = np.max(ensemblePredictions, axis=1)   \n",
        "    \n",
        "    #\n",
        "    classPredictions = np.argmax( classPredictions, axis=1 )      \n",
        "    predictions = testData.copy()\n",
        "    predictions['predictions'] = classPredictions \n",
        "    predictions = predictions['predictions']\n",
        "    predictions.to_csv( '{}/v0.1'.format(args.resultsPath ), index_label='ID', header=['label']   )\n",
        "##################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV4Uo6WQbGcU",
        "colab_type": "text"
      },
      "source": [
        "## Build Ensemble predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0PHjPYJbEwc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "baac394a-6b13-4dcd-e7c9-ed5e81269afa"
      },
      "source": [
        "\n",
        "## Generate test predictions per modelInstance\n",
        "for modelInstance in [ 'LR', 'RF_gini', 'AB_gini', 'NN0', 'NN1'  ]:\n",
        "    print(modelInstance)\n",
        "    genTestPredictionsPerModelInstance( modelInstance ) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-714827fa16a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodelInstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m'LR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RF_gini'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AB_gini'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN1'\u001b[0m  \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelInstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgenTestPredictionsPerModelInstance\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodelInstance\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-4ceb880db8e2>\u001b[0m in \u001b[0;36mgenTestPredictionsPerModelInstance\u001b[0;34m(modelInstance)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenTestPredictionsPerModelInstance\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodelInstance\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Generate predictions over test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfoldIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "filqY1dUbHx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Ensemble Predictions\n",
        "generateEnsemblePredictions( ensemble = [ 'LR', 'RF_gini', 'AB_gini', 'NN0', 'NN1' ] \n",
        "                           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEOZDavnbIuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}